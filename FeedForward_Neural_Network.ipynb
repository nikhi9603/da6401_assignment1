{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30918,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "RyBmHFH18zVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wandb numpy pandas matplotlib"
      ],
      "metadata": {
        "id": "ColG2_IA5yF7",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1: fashion-MNIST dataset"
      ],
      "metadata": {
        "id": "7OazPdR79C0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile Q1_fashion_mnist_class_images.py\n",
        "import wandb\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "classes = {\n",
        "    0: \"T-shirt/top\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle boot\"\n",
        "}\n",
        "\n",
        "def logClassImages(project_name:str):\n",
        "  wandb.init(project=project_name)\n",
        "  wandb_image_indices = []\n",
        "\n",
        "  for classNumber in range(10):\n",
        "    for j in range(len(y_test)):\n",
        "      if y_test[j] == classNumber:\n",
        "        wandb_image_indices.append(x_test[j])\n",
        "        break\n",
        "\n",
        "  wandb_images = [wandb.Image(wandb_image_indices[i], caption = classes[i]) for i in range(10)]\n",
        "  wandb.log({\"Sample images for each class\": wandb_images})\n",
        "  wandb.finish()\n",
        "\n",
        "logClassImages(\"da6401_assignment1\")"
      ],
      "metadata": {
        "id": "BX3u-Z-wedmf",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66e98ac4-719c-4d0a-e89c-c6d569e6e618"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Q1_fashion_mnist_class_images.py\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feedforward neural network\n",
        "\n"
      ],
      "metadata": {
        "id": "6Q1g4A4z5_k0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries required"
      ],
      "metadata": {
        "id": "GYt8MwnejIm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile libraries.py\n",
        "import wandb\n",
        "from keras.datasets import fashion_mnist, mnist\n",
        "import numpy\n",
        "import copy\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "import matplotlib.pyplot\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn\n",
        "import argparse"
      ],
      "metadata": {
        "id": "n-C62yxXtuau",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f24dab9f-013d-4462-d52a-4bb1956a4f4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting libraries.py\n"
          ]
        }
      ],
      "execution_count": 34
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activation functions"
      ],
      "metadata": {
        "id": "icnO6u_PiIUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile activation_functions.py\n",
        "import numpy as np\n",
        "\"\"\"\n",
        "  ACTIVATION FUNCTIONS\n",
        "\"\"\"\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "def sigmoid(x):\n",
        "    # x = np.clip(x,-10,10)\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def relu(x):\n",
        "    # print(x)\n",
        "    # x = np.clip(x, -200,200)\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))  # Numerical Stability\n",
        "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)"
      ],
      "metadata": {
        "id": "c1GLWE8XqDav",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c1d184d-d4f2-40ab-a5a4-9159942c8a54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting activation_functions.py\n"
          ]
        }
      ],
      "execution_count": 82
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Functions"
      ],
      "metadata": {
        "id": "OSZ_TdWoiMwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile loss_functions.py\n",
        "import numpy as np\n",
        "\"\"\"\n",
        "  LOSS FUNCTIONS\n",
        "\"\"\"\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    return -np.sum(y_true * np.log(y_pred + 1e-9), axis=-1)    # To avoid log 0, 1e-9 added to y_pred"
      ],
      "metadata": {
        "id": "oUg5E_A_vemb",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a5f61d5-1842-49f9-9033-8af221a775ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting loss_functions.py\n"
          ]
        }
      ],
      "execution_count": 54
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Derivatives"
      ],
      "metadata": {
        "id": "dxKR1JYIiQiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile derivatives.py\n",
        "import numpy as np\n",
        "# from activation_functions import *\n",
        "# from loss_functions import *\n",
        "\"\"\"\n",
        "  DERIVATIVES OF ACTIVATION AND LOSS FUNCTIONS\n",
        "\"\"\"\n",
        "def identity_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    sig = sigmoid(x)\n",
        "    return sig * (1 - sig)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1 - np.tanh(x) ** 2\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def mean_squared_error_derivative(y_true, y_pred):\n",
        "    return y_pred - y_true\n",
        "\n",
        "def cross_entropy_loss_derivative(y_true, y_pred):\n",
        "    return -y_true / (y_pred + 1e-9)\n",
        "\n",
        "def softmax_derivative(inp:np.array):\n",
        "    derivates = []\n",
        "    if(len(inp.shape) == 1):\n",
        "      S_vector = inp.reshape(-1, 1)\n",
        "      derivates = np.diag(inp) - np.dot(S_vector, S_vector.T)\n",
        "    elif(len(inp.shape) == 2):\n",
        "      for i in range(inp.shape[0]):\n",
        "        S_vector = inp[i].reshape(-1, 1)\n",
        "        derivates.append(np.diag(inp[i]) - np.dot(S_vector, S_vector.T))\n",
        "\n",
        "    return np.array(derivates)"
      ],
      "metadata": {
        "id": "z7G7FY4D_UN9",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04d80607-1f20-484b-b7c1-f543d5ff4c62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting derivatives.py\n"
          ]
        }
      ],
      "execution_count": 86
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizers"
      ],
      "metadata": {
        "id": "pw1-cdOZiWsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile optimizers.py\n",
        "import numpy as np\n",
        "\"\"\"\n",
        "  OPTIMIZERS UPDATE RULES\n",
        "\"\"\"\n",
        "\n",
        "# STOCHASTIC GRADIENT DESCENT\n",
        "def sgd(optimizer_input_dict, wts_bias_history_dict, itr=None):\n",
        "  # cant update weights in one single matrix op as dimensions of weights can be different in each layer\n",
        "  for i in range(optimizer_input_dict[\"n_hiddenLayers\"]+1):\n",
        "    # weight decay term added additionally to the formula in slides\n",
        "    wts_bias_history_dict[\"weights\"][i] = wts_bias_history_dict[\"weights\"][i] - optimizer_input_dict[\"learning_rate\"] * (wts_bias_history_dict[\"dw\"][i])\n",
        "    wts_bias_history_dict[\"biases\"][i] = wts_bias_history_dict[\"biases\"][i] - optimizer_input_dict[\"learning_rate\"] * (wts_bias_history_dict[\"db\"][i])\n",
        "\n",
        "# MOMENTUM BASED GRADIENT DESCENT\n",
        "def momentumGradientDescent(optimizer_input_dict, wts_bias_history_dict, itr=None):\n",
        "  for i in range(optimizer_input_dict[\"n_hiddenLayers\"]+1):\n",
        "    wts_bias_history_dict[\"history_weights\"][i] = (optimizer_input_dict[\"momentum\"] * wts_bias_history_dict[\"history_weights\"][i]) + wts_bias_history_dict[\"dw\"][i]\n",
        "    wts_bias_history_dict[\"weights\"][i] = wts_bias_history_dict[\"weights\"][i] - optimizer_input_dict[\"learning_rate\"] * (wts_bias_history_dict[\"history_weights\"][i])\n",
        "\n",
        "    wts_bias_history_dict[\"history_biases\"][i] = (optimizer_input_dict[\"momentum\"] * wts_bias_history_dict[\"history_biases\"][i]) + wts_bias_history_dict[\"db\"][i]\n",
        "    wts_bias_history_dict[\"biases\"][i] = wts_bias_history_dict[\"biases\"][i] - (optimizer_input_dict[\"learning_rate\"] * wts_bias_history_dict[\"history_biases\"][i])\n",
        "\n",
        "# NAG(NESTEROV ACCELERATED GRADIENT DESCENT)\n",
        "def nag(optimizer_input_dict, wts_bias_history_dict, itr=None):\n",
        "  for i in range(optimizer_input_dict[\"n_hiddenLayers\"]+1):\n",
        "    # dw,db will contain lookahead gradients only since forward and backward propagations are implemented accordingly\n",
        "    wts_bias_history_dict[\"history_weights\"][i] = (optimizer_input_dict[\"momentum\"] * wts_bias_history_dict[\"history_weights\"][i]) + wts_bias_history_dict[\"dw\"][i]\n",
        "    wts_bias_history_dict[\"weights\"][i] = wts_bias_history_dict[\"weights\"][i] - optimizer_input_dict[\"learning_rate\"] * (wts_bias_history_dict[\"history_weights\"][i])\n",
        "\n",
        "    wts_bias_history_dict[\"history_biases\"][i] = (optimizer_input_dict[\"momentum\"] * wts_bias_history_dict[\"history_biases\"][i]) + wts_bias_history_dict[\"db\"][i]\n",
        "    wts_bias_history_dict[\"biases\"][i] = wts_bias_history_dict[\"biases\"][i] - optimizer_input_dict[\"learning_rate\"] * wts_bias_history_dict[\"history_biases\"][i]\n",
        "\n",
        "# RMSPROP\n",
        "def rmsProp(optimizer_input_dict, wts_bias_history_dict, itr=None):\n",
        "  for i in range(optimizer_input_dict[\"n_hiddenLayers\"]+1):\n",
        "    wts_bias_history_dict[\"history_weights\"][i] = (optimizer_input_dict[\"beta\"] * wts_bias_history_dict[\"history_weights\"][i]) + (1 - optimizer_input_dict[\"beta\"]) * (wts_bias_history_dict[\"dw\"][i] ** 2)\n",
        "    # wts_bias_history_dict[\"weights\"][i] = wts_bias_history_dict[\"weights\"][i] - optimizer_input_dict[\"learning_rate\"] *((wts_bias_history_dict[\"dw\"][i]/np.sqrt(wts_bias_history_dict[\"history_weights\"][i] + optimizer_input_dict[\"epsilon\"])))  - (optimizer_input_dict[\"weight_decay\"] * wts_bias_history_dict[\"weights\"][i])\n",
        "    wts_bias_history_dict[\"weights\"][i] = wts_bias_history_dict[\"weights\"][i] - ((optimizer_input_dict[\"learning_rate\"]/np.sqrt(wts_bias_history_dict[\"history_weights\"][i] + optimizer_input_dict[\"epsilon\"])) * (wts_bias_history_dict[\"dw\"][i]))\n",
        "\n",
        "\n",
        "    wts_bias_history_dict[\"history_biases\"][i] = (optimizer_input_dict[\"beta\"] * wts_bias_history_dict[\"history_biases\"][i]) + (1 - optimizer_input_dict[\"beta\"]) * (wts_bias_history_dict[\"db\"][i] ** 2)\n",
        "    wts_bias_history_dict[\"biases\"][i] = wts_bias_history_dict[\"biases\"][i] - ((optimizer_input_dict[\"learning_rate\"]/np.sqrt(wts_bias_history_dict[\"history_biases\"][i] + optimizer_input_dict[\"epsilon\"])) * (wts_bias_history_dict[\"db\"][i]))\n",
        "\n",
        "# ADAM\n",
        "def adam(optimizer_input_dict, wts_bias_history_dict, itr=None):\n",
        "  for i in range(optimizer_input_dict[\"n_hiddenLayers\"]+1):\n",
        "    wts_bias_history_dict[\"history_weights\"][i] = (optimizer_input_dict[\"beta1\"] * wts_bias_history_dict[\"history_weights\"][i]) + (1 - optimizer_input_dict[\"beta1\"]) * (wts_bias_history_dict[\"dw\"][i])\n",
        "    wts_bias_history_dict[\"second_history_weights\"][i] = (optimizer_input_dict[\"beta2\"] * wts_bias_history_dict[\"second_history_weights\"][i]) + (1 - optimizer_input_dict[\"beta2\"]) * (wts_bias_history_dict[\"dw\"][i] ** 2)\n",
        "\n",
        "    history_weights_hat = wts_bias_history_dict[\"history_weights\"][i] / (1 - (optimizer_input_dict[\"beta1\"] ** (itr)))\n",
        "    second_history_weights_hat = wts_bias_history_dict[\"second_history_weights\"][i] / (1 - (optimizer_input_dict[\"beta2\"] ** (itr)))\n",
        "\n",
        "    # wts_bias_history_dict[\"weights\"][i] = wts_bias_history_dict[\"weights\"][i] - (optimizer_input_dict[\"learning_rate\"]*((history_weights_hat/(np.sqrt(second_history_weights_hat) + optimizer_input_dict[\"epsilon\"])))) - ((optimizer_input_dict[\"weight_decay\"] * wts_bias_history_dict[\"weights\"][i]))\n",
        "    wts_bias_history_dict[\"weights\"][i] = wts_bias_history_dict[\"weights\"][i] - ((optimizer_input_dict[\"learning_rate\"]/(np.sqrt(second_history_weights_hat) + optimizer_input_dict[\"epsilon\"])) * (history_weights_hat))\n",
        "\n",
        "    wts_bias_history_dict[\"history_biases\"][i] = (optimizer_input_dict[\"beta1\"] * wts_bias_history_dict[\"history_biases\"][i]) + (1 - optimizer_input_dict[\"beta1\"]) * wts_bias_history_dict[\"db\"][i]\n",
        "    wts_bias_history_dict[\"second_history_biases\"][i] = (optimizer_input_dict[\"beta2\"] * wts_bias_history_dict[\"second_history_biases\"][i]) + (1 - optimizer_input_dict[\"beta2\"]) * (wts_bias_history_dict[\"db\"][i] ** 2)\n",
        "\n",
        "    history_biases_hat = wts_bias_history_dict[\"history_biases\"][i] / (1 - (optimizer_input_dict[\"beta1\"] ** (itr)))\n",
        "    second_history_biases_hat = wts_bias_history_dict[\"second_history_biases\"][i] / (1 - (optimizer_input_dict[\"beta2\"] ** (itr)))\n",
        "\n",
        "    wts_bias_history_dict[\"biases\"][i] = wts_bias_history_dict[\"biases\"][i] - ((optimizer_input_dict[\"learning_rate\"]/(np.sqrt(second_history_biases_hat) + optimizer_input_dict[\"epsilon\"])) * (history_biases_hat))\n",
        "\n",
        "# NADAM\n",
        "def nadam(optimizer_input_dict, wts_bias_history_dict, itr=None):\n",
        "  for i in range(optimizer_input_dict[\"n_hiddenLayers\"]+1):\n",
        "    wts_bias_history_dict[\"history_weights\"][i] = (optimizer_input_dict[\"beta1\"] * wts_bias_history_dict[\"history_weights\"][i]) + (1 - optimizer_input_dict[\"beta1\"]) * (wts_bias_history_dict[\"dw\"][i])\n",
        "    wts_bias_history_dict[\"second_history_weights\"][i] = (optimizer_input_dict[\"beta2\"] * wts_bias_history_dict[\"second_history_weights\"][i]) + (1 - optimizer_input_dict[\"beta2\"]) * (wts_bias_history_dict[\"dw\"][i] ** 2)\n",
        "\n",
        "    history_weights_hat = wts_bias_history_dict[\"history_weights\"][i] / (1 - (optimizer_input_dict[\"beta1\"] ** itr))\n",
        "    second_history_weights_hat = wts_bias_history_dict[\"second_history_weights\"][i] / (1 - (optimizer_input_dict[\"beta2\"] ** itr))\n",
        "\n",
        "    lookahead_dw = optimizer_input_dict[\"beta1\"] * history_weights_hat + (((1-optimizer_input_dict[\"beta1\"])/(1-(optimizer_input_dict[\"beta1\"] ** itr))) * wts_bias_history_dict[\"dw\"][i])\n",
        "    # wts_bias_history_dict[\"weights\"][i] = wts_bias_history_dict[\"weights\"][i] - (optimizer_input_dict[\"learning_rate\"]*(lookahead_dw/(np.sqrt(second_history_weights_hat) + optimizer_input_dict[\"epsilon\"]))) - ((optimizer_input_dict[\"weight_decay\"] * wts_bias_history_dict[\"weights\"][i]))\n",
        "    wts_bias_history_dict[\"weights\"][i] = wts_bias_history_dict[\"weights\"][i] - ((optimizer_input_dict[\"learning_rate\"]/(np.sqrt(second_history_weights_hat) + optimizer_input_dict[\"epsilon\"])) * (lookahead_dw))\n",
        "\n",
        "\n",
        "    wts_bias_history_dict[\"history_biases\"][i] = (optimizer_input_dict[\"beta1\"] * wts_bias_history_dict[\"history_biases\"][i]) + (1 - optimizer_input_dict[\"beta1\"]) * wts_bias_history_dict[\"db\"][i]\n",
        "    wts_bias_history_dict[\"second_history_biases\"][i] = (optimizer_input_dict[\"beta2\"] * wts_bias_history_dict[\"second_history_biases\"][i]) + (1 - optimizer_input_dict[\"beta2\"]) * (wts_bias_history_dict[\"db\"][i] ** 2)\n",
        "\n",
        "    history_biases_hat = wts_bias_history_dict[\"history_biases\"][i] / (1 - (optimizer_input_dict[\"beta1\"] ** itr))\n",
        "    second_history_biases_hat = wts_bias_history_dict[\"second_history_biases\"][i] / (1 - (optimizer_input_dict[\"beta2\"] ** itr))\n",
        "\n",
        "    lookahead_db = optimizer_input_dict[\"beta1\"] * history_biases_hat + (((1-optimizer_input_dict[\"beta1\"])/(1-(optimizer_input_dict[\"beta1\"] ** itr))) * wts_bias_history_dict[\"db\"][i])\n",
        "    wts_bias_history_dict[\"biases\"][i] = wts_bias_history_dict[\"biases\"][i] - ((optimizer_input_dict[\"learning_rate\"]/(np.sqrt(second_history_biases_hat) + optimizer_input_dict[\"epsilon\"])) * (lookahead_db))\n"
      ],
      "metadata": {
        "id": "BuetfNNzUvjT",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c55dad50-8101-40f8-a6de-4c27157cf2b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting optimizers.py\n"
          ]
        }
      ],
      "execution_count": 56
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network (forward and back propagation)"
      ],
      "metadata": {
        "id": "TOWr8-ojiZ1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile neural_network.py\n",
        "# import libraries\n",
        "# from activation_functions import *\n",
        "# from loss_functions import *\n",
        "# from optimizers import *\n",
        "# from derivatives import *\n",
        "import numpy as np\n",
        "import copy\n",
        "import math\n",
        "\n",
        "class FeedForwardNeuralNetwork:\n",
        "  # class variables\n",
        "  optimizersMap = {\"sgd\": sgd, \"momentum\": momentumGradientDescent, \"nag\": nag, \"rmsprop\": rmsProp, \"adam\": adam, \"nadam\": nadam}\n",
        "  lossFunctionsMap = {\"mean_squared_error\": mean_squared_error, \"cross_entropy\" : cross_entropy_loss}\n",
        "  activationFunctionsMap = {\"identity\":identity, \"sigmoid\":sigmoid, \"tanh\":tanh, \"ReLU\":relu, \"softmax\": softmax}\n",
        "  derivatesFuncMap = {\"mean_squared_error\": mean_squared_error_derivative, \"cross_entropy_loss\": cross_entropy_loss_derivative, \"identity\": identity_derivative,\n",
        "                      \"sigmoid\": sigmoid_derivative, \"tanh\": tanh_derivative, \"relu\": relu_derivative, \"softmax\": softmax_derivative}\n",
        "\n",
        "  def __init__(self,\n",
        "               input_size=784, output_size=10,\n",
        "               n_hiddenLayers=3, n_neuronsPerLayer=32,\n",
        "               activationFun=\"sigmoid\",\n",
        "               weight_init=\"random\",\n",
        "               batch_size=64,\n",
        "               lossFunc=\"cross_entropy\",\n",
        "               optimizer=\"adam\",\n",
        "               learning_rate=0.001,\n",
        "               momentum=0.5,\n",
        "               beta=0.9, beta1=0.9, beta2=0.99,\n",
        "               epsilon=1e-8, weight_decay=0.01,\n",
        "               epochs=10):\n",
        "\n",
        "    # Inialtization parameters\n",
        "    self.input_size = input_size  # no of features\n",
        "    self.output_size = output_size\n",
        "    self.n_hiddenLayers = n_hiddenLayers\n",
        "    self.n_neuronsPerLayer = n_neuronsPerLayer\n",
        "    self.weight_init = weight_init\n",
        "    self.epochs = epochs\n",
        "\n",
        "    self.activationFun = FeedForwardNeuralNetwork.activationFunctionsMap[activationFun]\n",
        "    self.lossFunc = FeedForwardNeuralNetwork.lossFunctionsMap[lossFunc]\n",
        "    self.optimizer = FeedForwardNeuralNetwork.optimizersMap[optimizer]\n",
        "\n",
        "    # paramters required for optimizers\n",
        "    self.batch_size = batch_size\n",
        "    self.isLookAhead = False;\n",
        "\n",
        "    if(optimizer == \"nag\"):\n",
        "      self.isLookAhead = True;\n",
        "\n",
        "    # add these parameters as dict\n",
        "    self.optimizer_input_dict = { \"learning_rate\" : learning_rate,\n",
        "                                  \"momentum\" : momentum,                  # used by momentumGD\n",
        "                                  \"beta\" : beta,                          # used by rmsprop\n",
        "                                  \"beta1\" : beta1,                        # used by adam & nadam\n",
        "                                  \"beta2\" : beta2,                        # used by adam & nadam\n",
        "                                  \"epsilon\" : epsilon,\n",
        "                                  \"weight_decay\" : weight_decay,\n",
        "                                  \"n_hiddenLayers\": n_hiddenLayers}\n",
        "\n",
        "    # weights and biases matrices\n",
        "    self.weights = []\n",
        "    self.biases = []\n",
        "    self.lookAheadWeights = []\n",
        "    self.lookAheadBiases = []\n",
        "\n",
        "    self.wts_bias_history_dict = {\"weights\": self.weights, \"biases\": self.biases,\n",
        "                                  \"history_weights\": [np.zeros(1) for _ in range(self.n_hiddenLayers+1)],         # these will be modified before their first use (dimensions of each values will also be changed)\n",
        "                                  \"history_biases\": [np.zeros(1) for _ in range(self.n_hiddenLayers+1)],\n",
        "                                  \"second_history_weights\": [np.zeros(1) for _ in range(self.n_hiddenLayers+1)],\n",
        "                                  \"second_history_biases\": [np.zeros(1) for _ in range(self.n_hiddenLayers+1)],\n",
        "                                  \"dw\": [np.empty(1) for _ in range(self.n_hiddenLayers+1)],\n",
        "                                  \"dh\": [np.empty(1) for _ in range(self.n_hiddenLayers+1)]}\n",
        "\n",
        "    self.initializeWeightsAndBiases()\n",
        "    self.wts_bias_history_dict[\"second_history_weights\"] = copy.deepcopy(self.wts_bias_history_dict[\"history_weights\"])\n",
        "    self.wts_bias_history_dict[\"second_history_biases\"] = copy.deepcopy(self.wts_bias_history_dict[\"history_biases\"])\n",
        "\n",
        "    # pre-activation(a) and post-activation(h) values\n",
        "    self.a = []\n",
        "    self.h = []\n",
        "\n",
        "  '''\n",
        "    Weights,Biases initialization based on weight_init parameter\n",
        "\n",
        "    weights[0]: input layer to first hidden layer  : input_size x n_neuronsPerLayer\n",
        "    weights[1]: first hidden layer to second hidden layer : n_neuronsPerLayer x n_neuronsPerLayer\n",
        "    ...\n",
        "    weights[n_hiddenLayers]: last hidden layer to output layer : n_neuronsPerLayer x output_size\n",
        "\n",
        "    biases[i] : bias for ith layer : 1 x n_neuronsPerLayer   (i:0 to n_hiddenLayers-1)\n",
        "    biases[n_hiddenLayers]: 1 x output_size\n",
        "  '''\n",
        "  def initializeWeightsAndBiases(self):\n",
        "    # biases for both types\n",
        "    for i in range(self.n_hiddenLayers):\n",
        "      self.biases.append(np.zeros(self.n_neuronsPerLayer))\n",
        "      self.wts_bias_history_dict[\"history_biases\"][i] = np.zeros(self.n_neuronsPerLayer)\n",
        "\n",
        "    self.biases.append(np.zeros(self.output_size))   # biases[n_hiddenLayers]\n",
        "    self.wts_bias_history_dict[\"history_biases\"][self.n_hiddenLayers] = np.zeros(self.output_size)\n",
        "\n",
        "    if(self.weight_init == \"random\"):   # Random Normal\n",
        "      # weights[0]\n",
        "      self.weights.append(np.random.randn(self.input_size, self.n_neuronsPerLayer))\n",
        "      self.wts_bias_history_dict[\"history_weights\"][0] = np.zeros((self.input_size, self.n_neuronsPerLayer))\n",
        "\n",
        "      # weights[1] -> weights[n_hiddenLayers-1]\n",
        "      for i in range(self.n_hiddenLayers-1):\n",
        "        self.weights.append(np.random.randn(self.n_neuronsPerLayer, self.n_neuronsPerLayer))\n",
        "        self.wts_bias_history_dict[\"history_weights\"][i+1] = np.zeros((self.n_neuronsPerLayer, self.n_neuronsPerLayer))\n",
        "\n",
        "      # weights[n_hiddenLayers]\n",
        "      self.weights.append(np.random.randn(self.n_neuronsPerLayer, self.output_size))\n",
        "      self.wts_bias_history_dict[\"history_weights\"][self.n_hiddenLayers] = np.zeros((self.n_neuronsPerLayer, self.output_size))\n",
        "\n",
        "    elif(self.weight_init == \"Xavier\"):   # Xavier Normal: mean = 0, variance = 2/(n_input + n_output)\n",
        "      # weights[0]\n",
        "      self.weights.append(np.random.normal(loc=0.0, scale=np.sqrt(2/(self.input_size + self.n_neuronsPerLayer)), size=(self.input_size, self.n_neuronsPerLayer)))\n",
        "      self.wts_bias_history_dict[\"history_weights\"][0] = np.zeros((self.input_size, self.n_neuronsPerLayer))\n",
        "\n",
        "\n",
        "      for i in range(self.n_hiddenLayers-1):\n",
        "        self.weights.append(np.random.normal(loc=0.0, scale=np.sqrt(2/(self.n_neuronsPerLayer + self.n_neuronsPerLayer)), size=(self.n_neuronsPerLayer, self.n_neuronsPerLayer)))\n",
        "        self.wts_bias_history_dict[\"history_weights\"][i+1] = np.zeros((self.n_neuronsPerLayer, self.n_neuronsPerLayer))\n",
        "\n",
        "\n",
        "      self.weights.append(np.random.normal(loc=0.0, scale=np.sqrt(2/(self.n_neuronsPerLayer + self.output_size)), size=(self.n_neuronsPerLayer, self.output_size)))\n",
        "      self.wts_bias_history_dict[\"history_weights\"][self.n_hiddenLayers] = np.zeros((self.n_neuronsPerLayer, self.output_size))\n",
        "\n",
        "  '''\n",
        "    Forward propagation through the neural network. (for batch)\n",
        "    Instead of doing one input at a time, this function handles it for a batch using respective sized matrices\n",
        "\n",
        "    x_batch: B x n where B - batch size, n- no of features = input_size\n",
        "    x_batch is assumbed to be numpy array when given as input\n",
        "  '''\n",
        "  def forwardPropagation(self, x_batch, isValidation=False):\n",
        "    a_pre_activation = []\n",
        "    h_post_activation = []\n",
        "\n",
        "    # considering a0,h0 as X values as a1: first layer  (it is calculated from x values)\n",
        "    a_pre_activation.append(x_batch)\n",
        "    h_post_activation.append(x_batch)\n",
        "\n",
        "    wt = []\n",
        "    b = []\n",
        "\n",
        "    if(self.isLookAhead and not isValidation):\n",
        "      for i in range(self.n_hiddenLayers+1):\n",
        "        wt.append(self.weights[i] - (self.optimizer_input_dict[\"momentum\"] * self.wts_bias_history_dict[\"history_weights\"][i]))\n",
        "        b.append(self.biases[i] - (self.optimizer_input_dict[\"momentum\"] * self.wts_bias_history_dict[\"history_biases\"][i]))\n",
        "\n",
        "      self.lookAheadWeights = wt\n",
        "      self.lookAheadBiases = b\n",
        "    else:\n",
        "      wt = copy.deepcopy(self.weights)\n",
        "      b = copy.deepcopy(self.biases)\n",
        "\n",
        "    # Except last layer since activation function could be different\n",
        "    for i in range(self.n_hiddenLayers):\n",
        "      # ai: B x n_neuronsPerLayer, biases[i]: 1 x n_neuronsPerLayer (it will be broadcasted while adding)\n",
        "      ai = np.matmul(h_post_activation[-1], wt[i]) + b[i]\n",
        "      hi = self.activationFun(ai)\n",
        "\n",
        "      a_pre_activation.append(ai)\n",
        "      h_post_activation.append(hi)\n",
        "\n",
        "    # aL: last layer (activation function is softmax)\n",
        "    aL = np.matmul(h_post_activation[-1], wt[self.n_hiddenLayers]) + b[self.n_hiddenLayers]\n",
        "    hL = softmax(aL)   # y_batch\n",
        "\n",
        "    a_pre_activation.append(aL)\n",
        "    h_post_activation.append(hL)\n",
        "\n",
        "    return a_pre_activation, h_post_activation\n",
        "\n",
        "  '''\n",
        "    Backward propagation through the neural network. (for batch)\n",
        "  '''\n",
        "  def backwardPropagation(self, a_pre_activation, h_post_activation, y_batch, y_pred_batch):\n",
        "    grad_w = []\n",
        "    grad_b = []\n",
        "    grad_a = []\n",
        "    grad_h = []\n",
        "\n",
        "    wt = []\n",
        "    b = []\n",
        "    if(self.isLookAhead):\n",
        "        wt = self.lookAheadWeights\n",
        "        b = self.lookAheadBiases\n",
        "    else:\n",
        "        wt = copy.deepcopy(self.weights)\n",
        "        b = copy.deepcopy(self.biases)\n",
        "\n",
        "    # Output gradient (wrt aL)\n",
        "    grad_hL = self.derivatesFuncMap[self.lossFunc.__name__](y_batch, y_pred_batch)\n",
        "    grad_h.append(grad_hL)\n",
        "\n",
        "    if(self.lossFunc.__name__ == \"cross_entropy_loss\"):\n",
        "      grad_aL = y_pred_batch - y_batch    # just to reduce computation of jacobian matrix\n",
        "      grad_a.append(grad_aL)\n",
        "    else:\n",
        "      grad_aL_list = []\n",
        "      # softmax derivatives of each input is a matrix of size output_size x output_size, we need to perform matrix_mul for each input of batch\n",
        "      for i in range(y_batch.shape[0]):   # self.batch_size = y_batch.shape[0] but better to take y_batch.shape[0] since last batch inputs can have less\n",
        "        grad_aL_inp_i = grad_hL[i] @ softmax_derivative(y_pred_batch[i])\n",
        "        grad_aL_list.append(grad_aL_inp_i)\n",
        "\n",
        "      grad_aL = np.array(grad_aL_list)\n",
        "      grad_aL = grad_aL / y_batch.shape[0]\n",
        "      grad_a.append(grad_aL)                    # aL contains (aL) values of all inputs in the batch\n",
        "\n",
        "    # Hidden layers\n",
        "    for k in range(self.n_hiddenLayers, -1, -1):\n",
        "      # gradients w.r.t parameters\n",
        "      # wk\n",
        "      grad_wk = np.zeros_like(wt[k])    # will be equal to sum across\n",
        "\n",
        "      for inpNum in range(y_batch.shape[0]):\n",
        "        grad_wk_inp_num = np.matmul(h_post_activation[k][inpNum].reshape(-1,1), grad_a[-1][inpNum].reshape(1,-1))\n",
        "        grad_wk += grad_wk_inp_num\n",
        "      grad_w.append(grad_wk)                   # contains sum across all batches\n",
        "\n",
        "      # bk\n",
        "      grad_bk = np.zeros_like(self.biases[k])\n",
        "      for inpNum in range(y_batch.shape[0]):\n",
        "        grad_bk += grad_a[-1][inpNum]\n",
        "      grad_b.append(grad_bk)                     # contains sum across all batches\n",
        "\n",
        "      if(k > 0):\n",
        "        # gradients w.r.t layer below\n",
        "        grad_hk_1 = grad_a[-1] @ wt[k].T\n",
        "        grad_h.append(grad_hk_1)\n",
        "\n",
        "        # gradients w.r.t layer below (pre-activation)\n",
        "        grad_ak_1 = grad_hk_1 * self.derivatesFuncMap[self.activationFun.__name__](a_pre_activation[k])\n",
        "        grad_a.append(grad_ak_1)\n",
        "\n",
        "    grad_w = grad_w[::-1]\n",
        "    grad_b = grad_b[::-1]\n",
        "\n",
        "    for i in range(self.n_hiddenLayers):\n",
        "        grad_w[i] = grad_w[i] + (self.optimizer_input_dict[\"weight_decay\"] * wt[i])\n",
        "\n",
        "    return grad_w, grad_b\n",
        "\n",
        "  def updateWeights(self, grad_w, grad_b, itr):\n",
        "    grad_w = [np.clip(dw, -10,10) for dw in grad_w]\n",
        "    grad_h = [np.clip(db, -10,10) for db in grad_b]\n",
        "    self.wts_bias_history_dict[\"dw\"] = grad_w\n",
        "    self.wts_bias_history_dict[\"db\"] = grad_b\n",
        "    self.optimizer(self.optimizer_input_dict, self.wts_bias_history_dict, itr)"
      ],
      "metadata": {
        "id": "OAFNdkiD6ORe",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b75f72ef-a162-4aa3-f2fd-b61217feabb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting neural_network.py\n"
          ]
        }
      ],
      "execution_count": 84
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading data"
      ],
      "metadata": {
        "id": "Sn4gQPUbjAXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile dataset_load.py\n",
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist, mnist\n",
        "# import numpy\n",
        "\n",
        "datasets = {\"fashion_mnist\": fashion_mnist, \"mnist\": mnist}\n",
        "\n",
        "def load_data(dataset_name):\n",
        "  (x_train, y_train), (x_test, y_test) = datasets[dataset_name].load_data()\n",
        "  num_classes = len(np.unique(y_train))\n",
        "\n",
        "  y_train = np.eye(num_classes)[y_train]\n",
        "  y_test = np.eye(num_classes)[y_test]\n",
        "\n",
        "  x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "  x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "  x_train = np.array(x_train/255, dtype=np.float64)\n",
        "  y_train = np.array(y_train, dtype=np.float64)\n",
        "  x_test = np.array(x_test/255, dtype=np.float64)\n",
        "  y_test = np.array(y_test, dtype=np.float64)\n",
        "\n",
        "  return x_train, y_train, x_test, y_test, num_classes"
      ],
      "metadata": {
        "id": "NrmZMAZQjGDg",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f332ce09-41f0-4c12-f157-0e73002a3834"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting dataset_load.py\n"
          ]
        }
      ],
      "execution_count": 58
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sweep Configuration"
      ],
      "metadata": {
        "id": "F_oOFu-I5UAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_configuration = {\n",
        "    \"method\": \"bayes\",\n",
        "    \"name\" : \"final_sweep\",\n",
        "    \"metric\": {\"name\": \"validation_accuracy\", \"goal\": \"maximize\"},\n",
        "    \"parameters\": {\n",
        "        \"learning_rate\": {\"values\": [1e-3, 1e-4]},\n",
        "        \"optimizer\": {\"values\": [\"sgd\", \"momentum\", \"nag\", \"rmsprop\",  \"adam\", \"nadam\"]},\n",
        "        \"num_layers\": {\"values\": [3, 4, 5]},\n",
        "        \"hidden_size\": {\"values\": [32, 64, 128]},\n",
        "        \"batch_size\": {\"values\": [16, 32, 64]},\n",
        "        \"activation\": {\"values\": [\"sigmoid\", \"tanh\", \"ReLU\"]},\n",
        "        \"weight_decay\": {\"values\": [0, 0.0005, 0.5]},\n",
        "        \"weight_init\": {\"values\": [\"random\", \"Xavier\"]},\n",
        "        \"epochs\": {\"values\": [10, 5]},\n",
        "        \"loss\": {\"values\": [\"cross_entropy\"]},\n",
        "        \"momentum\": {\"values\": [0.9]},\n",
        "        \"beta\": {\"values\": [0.9]},\n",
        "        \"beta1\": {\"values\":[0.9]},\n",
        "        \"beta2\": {\"values\": [0.999]},\n",
        "        \"epsilon\": {\"values\": [1e-8]},\n",
        "        \"dataset\": {\"values\":[\"fashion_mnist\"]}\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "GDFh8x1N5TZW",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# sweep_configuration = {\n",
        "#     \"method\": \"grid\",\n",
        "#     \"name\" : \"fashion_mnist_nadam_relu_confusion_matrix\",\n",
        "#     \"parameters\": {\n",
        "#         \"learning_rate\": {\"values\": [1e-3]},\n",
        "#         \"optimizer\": {\"values\": [\"nadam\"]},\n",
        "#         \"num_layers\": {\"values\": [4]},\n",
        "#         \"hidden_size\": {\"values\": [128]},\n",
        "#         \"batch_size\": {\"values\": [16]},\n",
        "#         \"activation\": {\"values\": [\"ReLU\"]},\n",
        "#         \"weight_decay\": {\"values\": [0]},\n",
        "#         \"weight_init\": {\"values\": [\"Xavier\"]},\n",
        "#         \"epochs\": {\"values\": [10]},\n",
        "#         \"loss\": {\"values\": [\"cross_entropy\"]},\n",
        "#         \"momentum\": {\"values\": [0.9]},\n",
        "#         \"beta\": {\"values\": [0.9]},\n",
        "#         \"beta1\": {\"values\":[0.9]},\n",
        "#         \"beta2\": {\"values\": [0.999]},\n",
        "#         \"epsilon\": {\"values\": [1e-8]},\n",
        "#         \"dataset\": {\"values\":[\"fashion_mnist\"]},\n",
        "#         \"isConfusionMatrix\": {\"values\": [\"True\"]}\n",
        "#     }\n",
        "# }"
      ],
      "metadata": {
        "trusted": true,
        "id": "xTvtyIWIr_RA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Feed Forward Neural Network"
      ],
      "metadata": {
        "id": "2DJr_18yixYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile train_sweep.py\n",
        "# from neural_network import *\n",
        "# from dataset_load import *\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import wandb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "sweep_configuration = {\n",
        "    \"method\": \"bayes\",\n",
        "    \"name\" : \"train_sweep\",\n",
        "    \"metric\": {\"name\": \"validation_accuracy\", \"goal\": \"maximize\"},\n",
        "    \"parameters\": {\n",
        "        \"learning_rate\": {\"values\": [1e-3, 1e-4]},\n",
        "        \"optimizer\": {\"values\": [\"sgd\", \"momentum\", \"nag\", \"rmsprop\",  \"adam\", \"nadam\"]},\n",
        "        \"num_layers\": {\"values\": [3, 4, 5]},\n",
        "        \"hidden_size\": {\"values\": [32, 64, 128]},\n",
        "        \"batch_size\": {\"values\": [16, 32, 64]},\n",
        "        \"activation\": {\"values\": [\"sigmoid\", \"tanh\", \"ReLU\"]},\n",
        "        \"weight_decay\": {\"values\": [0, 0.0005, 0.5]},\n",
        "        \"weight_init\": {\"values\": [\"random\", \"Xavier\"]},\n",
        "        \"epochs\": {\"values\": [10, 5]},\n",
        "        \"loss\": {\"values\": [\"cross_entropy\"]},\n",
        "        \"momentum\": {\"values\": [0.9]},\n",
        "        \"beta\": {\"values\": [0.9]},\n",
        "        \"beta1\": {\"values\":[0.9]},\n",
        "        \"beta2\": {\"values\": [0.999]},\n",
        "        \"epsilon\": {\"values\": [1e-8]},\n",
        "        \"dataset\": {\"values\":[\"fashion_mnist\"]}\n",
        "    }\n",
        "}\n",
        "\n",
        "def calculateAccuracy(y_true, y_pred):\n",
        "  y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "  y_true_labels = np.argmax(y_true, axis=1)\n",
        "  accuracy = np.mean(y_pred_labels == y_true_labels)\n",
        "  return accuracy*100\n",
        "\n",
        "def trainNeuralNetwork_sweep():\n",
        "  wandb.init(mode=\"online\")\n",
        "  args = wandb.config\n",
        "  x_train, y_train, x_test, y_test, num_classes = load_data(args[\"dataset\"])\n",
        "  input_size = len(x_train[0])\n",
        "  output_size = num_classes\n",
        "  n_hiddenLayers = args[\"num_layers\"]\n",
        "  n_neuronsPerLayer = args[\"hidden_size\"]\n",
        "  activationFun = args[\"activation\"]\n",
        "  weight_init = args[\"weight_init\"]\n",
        "  batch_size = args[\"batch_size\"]\n",
        "  lossFunc = args[\"loss\"]\n",
        "  optimizer = args[\"optimizer\"]\n",
        "  learning_rate = args[\"learning_rate\"]\n",
        "  momentum = args[\"momentum\"]\n",
        "  beta = args[\"beta\"]\n",
        "  beta1 = args[\"beta1\"]\n",
        "  beta2 = args[\"beta2\"]\n",
        "  epsilon = args[\"epsilon\"]\n",
        "  weight_decay = args[\"weight_decay\"]\n",
        "  epochs = args[\"epochs\"]\n",
        "\n",
        "  wandb.run.name = f\"{optimizer}_{activationFun}_{n_hiddenLayers}_{n_neuronsPerLayer}_{epochs}_{weight_init}\"\n",
        "\n",
        "  # paste all above paramters as fun params\n",
        "  fnn = FeedForwardNeuralNetwork(input_size, output_size, n_hiddenLayers, n_neuronsPerLayer,\n",
        "                                 activationFun, weight_init, batch_size, lossFunc,\n",
        "                                 optimizer, learning_rate, momentum,\n",
        "                                 beta, beta1, beta2,\n",
        "                                 epsilon, weight_decay, epochs)\n",
        "\n",
        "  x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
        "  num_batches = math.ceil(len(x_train)/batch_size)\n",
        "\n",
        "  for epochNum in range(epochs):\n",
        "    for batchNum in range(num_batches):\n",
        "      start_idx = batchNum * batch_size\n",
        "      end_idx = start_idx + batch_size\n",
        "\n",
        "      x_batch = x_train[start_idx:end_idx]\n",
        "      y_batch = y_train[start_idx:end_idx]\n",
        "\n",
        "      # Forward Propagation\n",
        "      a_pre_activation, h_post_activation = fnn.forwardPropagation(x_batch)\n",
        "      y_pred_batch = h_post_activation[-1]\n",
        "\n",
        "      # Back Propagation\n",
        "      grad_w, grad_b = fnn.backwardPropagation(a_pre_activation, h_post_activation, y_batch, y_pred_batch)\n",
        "\n",
        "      # Update weights\n",
        "      itr = epochNum * num_batches + batchNum + 1\n",
        "      fnn.updateWeights(grad_w, grad_b, itr)\n",
        "\n",
        "    # Validation accuracy\n",
        "    _, h_validation = fnn.forwardPropagation(x_validation, isValidation=True)\n",
        "    y_pred_validation = h_validation[-1]\n",
        "    validation_accuracy = calculateAccuracy(y_validation, y_pred_validation)\n",
        "    wandb.run.summary[\"metric_name\"] = validation_accuracy\n",
        "\n",
        "\n",
        "    # Train accuracy\n",
        "    _, h_train = fnn.forwardPropagation(x_train, isValidation=True)\n",
        "    y_pred_train = h_train[-1]\n",
        "    train_accuracy = calculateAccuracy(y_train, y_pred_train)\n",
        "\n",
        "    wandb.log({\n",
        "        \"epoch\": epochNum + 1,\n",
        "        \"validation_loss\": np.mean(fnn.lossFunc(y_validation, y_pred_validation)),\n",
        "        \"validation_accuracy\": validation_accuracy,\n",
        "        \"train_loss\": np.mean(fnn.lossFunc(y_train, y_pred_train)),\n",
        "        \"train_accuracy\": train_accuracy\n",
        "        },commit=True)\n",
        "\n",
        "  # Test accuracy\n",
        "  _,h_test = fnn.forwardPropagation(x_test, isValidation=True)\n",
        "  y_pred_test = h_test[-1]\n",
        "  test_accuracy = calculateAccuracy(y_test, y_pred_test)\n",
        "  wandb.log({ \"test_accuracy\": test_accuracy,\n",
        "             \"test_loss\": np.mean(fnn.lossFunc(y_test, y_pred_test))})\n",
        "\n",
        "  # Confusion matrix\n",
        "  class_names = []\n",
        "  if(args[\"isConfusionMatrix\"] == \"True\"):\n",
        "      if(args[\"dataset\"] == \"fashion_mnist\"):\n",
        "          class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\",\"Coat\",\"Sandal\", \"Shirt\", \"Sneaker\",\"Bag\",\"Ankle boot\"]\n",
        "      elif(args[\"dataset\"] == \"mnist\"):\n",
        "          class_names = [str(i) for i in range(10)]\n",
        "\n",
        "  confusion_mat = confusion_matrix(y_pred_test.argmax(axis=1), y_test.argmax(axis=1))\n",
        "\n",
        "  # plot\n",
        "  plt.figure(figsize=(8,8))\n",
        "  sns.heatmap(confusion_mat, annot=True, fmt=\"d\", xticklabels=class_names, yticklabels=class_names, cmap=\"Greens\")\n",
        "  plt.xlabel(\"y_true\")\n",
        "  plt.ylabel(\"y_pred\")\n",
        "  plt.title(\"Confusion Matrix\")\n",
        "\n",
        "  wandb.log({\"confusion_matrix\": wandb.Image(plt)})\n",
        "  plt.close()\n",
        "\n",
        "  wandb.finish()\n",
        "\n",
        "wandb.login()\n",
        "wandb_id = wandb.sweep(sweep_configuration, project=\"DA6401_Assignment1\")\n",
        "wandb.agent(wandb_id, function=trainNeuralNetwork_sweep)"
      ],
      "metadata": {
        "id": "C5Hv_3X6i3UP",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49740fc2-8593-48e3-ea92-c9e319030941"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train_sweep.py\n"
          ]
        }
      ],
      "execution_count": 77
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY=\"x\"\n",
        "wandb.login(key=API_KEY)"
      ],
      "metadata": {
        "id": "eJBoKRNWKqyS",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Squared error loss config"
      ],
      "metadata": {
        "id": "QiT8BI32r_RC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_configuration_2 = {\n",
        "    \"method\": \"grid\",\n",
        "    \"name\" : \"squared_error\",\n",
        "    \"parameters\": {\n",
        "        \"learning_rate\": {\"values\": [1e-3]},\n",
        "        \"num_layers\": {\"values\": [4]},\n",
        "        \"hidden_size\": {\"values\": [128]},\n",
        "        \"batch_size\": {\"values\": [16]},\n",
        "        \"weight_decay\": {\"values\": [0]},\n",
        "        \"weight_init\": {\"values\": [\"Xavier\"]},\n",
        "        \"epochs\": {\"values\": [10]},\n",
        "        \"loss\": {\"values\": [\"mean_squared_error\", \"cross_entropy\"]},\n",
        "        \"optimizer\": {\"values\": [\"nadam\", \"sgd\"]},\n",
        "        \"activation\": {\"values\": [\"ReLU\", \"tanh\", \"sigmoid\"]},\n",
        "        \"momentum\": {\"values\": [0.9]},\n",
        "        \"beta\": {\"values\": [0.9]},\n",
        "        \"beta1\": {\"values\":[0.9]},\n",
        "        \"beta2\": {\"values\": [0.999]},\n",
        "        \"epsilon\": {\"values\": [1e-8]},\n",
        "        \"dataset\": {\"values\":[\"fashion_mnist\"]},\n",
        "        \"isConfusionMatrix\": {\"values\": [\"False\"]}\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "trusted": true,
        "id": "HcMP4rAkr_RC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "wandb_id = wandb.sweep(sweep_configuration_2, project=\"DA6401_Assignment1\")\n",
        "wandb.agent(wandb_id, function=trainNeuralNetwork_sweep)"
      ],
      "metadata": {
        "trusted": true,
        "id": "QevLcO1hr_RC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Argument Parser"
      ],
      "metadata": {
        "id": "P7ud_YIer_RC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile argument_parser.py\n",
        "import argparse\n",
        "\n",
        "def parse_arguments():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\"-wp\", \"--wandb_project\", type=str, default=\"DA6401_Assignment1\",\n",
        "                        help=\"Project name used to track experiments in Weights & Biases dashboard\")\n",
        "    parser.add_argument(\"-we\", \"--wandb_entity\", type=str, default=\"nikhithaa-iit-madras\",\n",
        "                        help=\"Wandb Entity used to track experiments in the Weights & Biases dashboard.\")\n",
        "    parser.add_argument(\"-d\", \"--dataset\", type=str, choices=[\"mnist\", \"fashion_mnist\"], default=\"fashion_mnist\",\n",
        "                        help=\"Choose one among these datasets: ['mnist', 'fashion_mnist']\")\n",
        "    parser.add_argument(\"-e\", \"--epochs\", type=int, default=10,\n",
        "                        help=\"Number of epochs to train neural network\")\n",
        "    parser.add_argument(\"-b\", \"--batch_size\", type=int, default=16,\n",
        "                        help=\"Batch size used to train neural network\")\n",
        "    parser.add_argument(\"-l\", \"--loss\", type=str, choices=[\"mean_squared_error\", \"cross_entropy\"], default=\"cross_entropy\",\n",
        "                        help=\"Choose one among these loss functions: ['mean_squared_error', 'cross_entropy']\")\n",
        "    parser.add_argument(\"-o\", \"--optimizer\", type=str, choices=[\"sgd\", \"momentum\", \"nag\", \"rmsprop\", \"adam\", \"nadam\"], default=\"nadam\",\n",
        "                        help=\"Choose one among these optimizers: ['sgd', 'momentum', 'nag', 'rmsprop', 'adam', 'nadam']\")\n",
        "    parser.add_argument(\"-lr\", \"--learning_rate\", type=float, default=0.001,\n",
        "                        help=\"Learning rate used to optimize model parameters\")\n",
        "    parser.add_argument(\"-m\", \"--momentum\", type=float, default=0.9,\n",
        "                        help=\"Momentum used by momentum and nag optimizers\")\n",
        "    parser.add_argument(\"-beta\", \"--beta\", type=float, default=0.9,\n",
        "                        help=\"Beta used by rmsprop optimizer\")\n",
        "    parser.add_argument(\"-beta1\", \"--beta1\", type=float, default=0.9,\n",
        "                        help=\"Beta1 used by adam and nadam optimizers\")\n",
        "    parser.add_argument(\"-beta2\", \"--beta2\", type=float, default=0.999,\n",
        "                        help=\"Beta2 used by adam and nadam optimizers\")\n",
        "    parser.add_argument(\"-eps\", \"--epsilon\", type=float, default=0.00000001,\n",
        "                        help=\"Epsilon used by optimizers\")\n",
        "    parser.add_argument(\"-w_d\", \"--weight_decay\", type=float, default=0.0005,\n",
        "                        help=\"Weight decay used by optimizers\")\n",
        "    parser.add_argument(\"-w_i\", \"--weight_init\", type=str, choices=[\"random\", \"Xavier\"], default=\"Xavier\",\n",
        "                        help=\"Choose one among these weight initialization methods: ['random', 'Xavier']\")\n",
        "    parser.add_argument(\"-nhl\", \"--num_layers\", type=int, default=4,\n",
        "                        help=\"Number of hidden layers used in feedforward neural network\")\n",
        "    parser.add_argument(\"-sz\", \"--hidden_size\", type=int, default=64,\n",
        "                        help=\"Number of hidden neurons in a feedforward layer\")\n",
        "    parser.add_argument(\"-a\", \"--activation\", type=str, choices=[\"identity\", \"sigmoid\", \"tanh\", \"ReLU\"], default=\"sigmoid\",\n",
        "                        help=\"Choose one among these activation functions: ['identity', 'sigmoid', 'tanh', 'ReLU']\")\n",
        "    parser.add_argument(\"-cm\", \"--confusion_matrix\", type=str, choices=[\"True\", \"False\"], default=\"False\",\n",
        "                        help=\"Set true if confusion matrix to be logged\")\n",
        "\n",
        "    return parser.parse_args()\n"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUr71y2Mr_RD",
        "outputId": "21856323-e6f4-4040-f1ef-9eb7ae680372"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting argument_parser.py\n"
          ]
        }
      ],
      "execution_count": 98
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training (argparser included) for train.py file"
      ],
      "metadata": {
        "id": "f_BooQPxr_RD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile train.py\n",
        "# from neural_network import *\n",
        "# from dataset_load import *\n",
        "# from argument_parser import *\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import wandb\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def calculateAccuracy(y_true, y_pred):\n",
        "  y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "  y_true_labels = np.argmax(y_true, axis=1)\n",
        "  accuracy = np.mean(y_pred_labels == y_true_labels)\n",
        "  return accuracy*100\n",
        "\n",
        "\n",
        "def trainNeuralNetwork(args):\n",
        "  wandb.login()\n",
        "  wandb.init(project=args.wandb_project, entity=args.wandb_entity)\n",
        "  x_train, y_train, x_test, y_test, num_classes = load_data(args.dataset)\n",
        "  input_size = len(x_train[0])\n",
        "  output_size = num_classes\n",
        "  n_hiddenLayers = args.num_layers\n",
        "  n_neuronsPerLayer = args.hidden_size\n",
        "  activationFun = args.activation\n",
        "  weight_init = args.weight_init\n",
        "  batch_size = args.batch_size\n",
        "  lossFunc = args.loss\n",
        "  optimizer = args.optimizer\n",
        "  learning_rate = args.learning_rate\n",
        "  momentum = args.momentum\n",
        "  beta = args.beta\n",
        "  beta1 = args.beta1\n",
        "  beta2 = args.beta2\n",
        "  epsilon = args.epsilon\n",
        "  weight_decay = args.weight_decay\n",
        "  epochs = args.epochs\n",
        "\n",
        "  wandb.run.name = f\"train_run_{optimizer}_{activationFun}_{n_hiddenLayers}_{n_neuronsPerLayer}_{epochs}_{weight_init}\"\n",
        "\n",
        "  # paste all above paramters as fun params\n",
        "  fnn = FeedForwardNeuralNetwork(input_size, output_size, n_hiddenLayers, n_neuronsPerLayer,\n",
        "                                 activationFun, weight_init, batch_size, lossFunc,\n",
        "                                 optimizer, learning_rate, momentum,\n",
        "                                 beta, beta1, beta2,\n",
        "                                 epsilon, weight_decay, epochs)\n",
        "\n",
        "  x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
        "  num_batches = math.ceil(len(x_train)/batch_size)\n",
        "\n",
        "  for epochNum in range(epochs):\n",
        "    for batchNum in range(num_batches):\n",
        "      start_idx = batchNum * batch_size\n",
        "      end_idx = start_idx + batch_size\n",
        "\n",
        "      x_batch = x_train[start_idx:end_idx]\n",
        "      y_batch = y_train[start_idx:end_idx]\n",
        "\n",
        "      # Forward Propagation\n",
        "      a_pre_activation, h_post_activation = fnn.forwardPropagation(x_batch)\n",
        "      y_pred_batch = h_post_activation[-1]\n",
        "\n",
        "      # Back Propagation\n",
        "      grad_w, grad_b = fnn.backwardPropagation(a_pre_activation, h_post_activation, y_batch, y_pred_batch)\n",
        "\n",
        "      # Update weights\n",
        "      itr = epochNum * num_batches + batchNum + 1\n",
        "      fnn.updateWeights(grad_w, grad_b, itr)\n",
        "\n",
        "    # Validation accuracy\n",
        "    _, h_validation = fnn.forwardPropagation(x_validation, isValidation=True)\n",
        "    y_pred_validation = h_validation[-1]\n",
        "    validation_accuracy = calculateAccuracy(y_validation, y_pred_validation)\n",
        "    wandb.run.summary[\"metric_name\"] = validation_accuracy\n",
        "\n",
        "\n",
        "    # Train accuracy\n",
        "    _, h_train = fnn.forwardPropagation(x_train, isValidation=True)\n",
        "    y_pred_train = h_train[-1]\n",
        "    train_accuracy = calculateAccuracy(y_train, y_pred_train)\n",
        "\n",
        "    wandb.log({\n",
        "        \"epoch\": epochNum + 1,\n",
        "        \"validation_loss\": np.mean(fnn.lossFunc(y_validation, y_pred_validation)),\n",
        "        \"validation_accuracy\": validation_accuracy,\n",
        "        \"train_loss\": np.mean(fnn.lossFunc(y_train, y_pred_train)),\n",
        "        \"train_accuracy\": train_accuracy\n",
        "        },commit=True)\n",
        "\n",
        "  # Test accuracy\n",
        "  _,h_test = fnn.forwardPropagation(x_test, isValidation=True)\n",
        "  y_pred_test = h_test[-1]\n",
        "  test_accuracy = calculateAccuracy(y_test, y_pred_test)\n",
        "  wandb.log({ \"test_accuracy\": test_accuracy,\n",
        "             \"test_loss\": np.mean(fnn.lossFunc(y_test, y_pred_test))})\n",
        "\n",
        "  # Confusion matrix\n",
        "  class_names = []\n",
        "  if(args.confusion_matrix == \"True\"):\n",
        "      if(args.dataset == \"fashion_mnist\"):\n",
        "          class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\",\"Coat\",\"Sandal\", \"Shirt\", \"Sneaker\",\"Bag\",\"Ankle boot\"]\n",
        "      elif(args.dataset == \"mnist\"):\n",
        "          class_names = [str(i) for i in range(10)]\n",
        "\n",
        "      confusion_mat = confusion_matrix(y_pred_test.argmax(axis=1), y_test.argmax(axis=1))\n",
        "\n",
        "      # plot\n",
        "      plt.figure(figsize=(8,8))\n",
        "      sns.heatmap(confusion_mat, annot=True, fmt=\"d\", xticklabels=class_names, yticklabels=class_names, cmap=\"Greens\")\n",
        "      plt.xlabel(\"y_true\")\n",
        "      plt.ylabel(\"y_pred\")\n",
        "      plt.title(\"Confusion Matrix\")\n",
        "      plt.xticks(rotation=45)\n",
        "      plt.yticks(rotation=45)\n",
        "      plt.tight_layout()\n",
        "\n",
        "      wandb.log({\"confusion_matrix\": wandb.Image(plt)})\n",
        "      plt.close()\n",
        "\n",
        "  wandb.finish()\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "  args = parse_arguments()\n",
        "  trainNeuralNetwork(args)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJ8YV_NDr_RD",
        "outputId": "84e80bf5-6a3f-4fa9-e843-4d94fa90b483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train.py\n"
          ]
        }
      ],
      "execution_count": 104
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 train.py -wp DA6401_Assignment1 -we nikhithaa-iit-madras -b 16 -beta1 0.9 -beta2 0.999 -lr 0.001 -e 10 --dataset fashion_mnist -o nadam -a ReLU -w_d 0 -cm True -nhl 4 -sz 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWkUyrqF3YYa",
        "outputId": "426e2747-e354-44a2-9319-b52a04d8e7ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-17 18:03:34.367855: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742234614.394038   39909 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742234614.401654   39909 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnikhithaa\u001b[0m (\u001b[33mnikhithaa-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250317_180339-9uxj2zq5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlucky-pyramid-1327\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment1/runs/9uxj2zq5\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY=\"x\"\n",
        "wandb.login(key=API_KEY)"
      ],
      "metadata": {
        "id": "H_uKzZHK7TZj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
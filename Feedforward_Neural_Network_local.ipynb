{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "RyBmHFH18zVt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ColG2_IA5yF7",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "pip install wandb numpy pandas matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1: fashion-MNIST dataset"
      ],
      "metadata": {
        "id": "7OazPdR79C0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "classes = {\n",
        "    0: \"T-shirt/top\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle boot\"\n",
        "}\n",
        "\n",
        "def logClassImages(project_name:str):\n",
        "  wandb.init(project=project_name)\n",
        "  wandb_image_indices = []\n",
        "\n",
        "  for classNumber in range(10):\n",
        "    for j in range(len(y_test)):\n",
        "      if y_test[j] == classNumber:\n",
        "        wandb_image_indices.append(x_test[j])\n",
        "        break\n",
        "\n",
        "  wandb_images = [wandb.Image(wandb_image_indices[i], caption = classes[i]) for i in range(10)]\n",
        "  wandb.log({\"Sample images for each class\": wandb_images})\n",
        "  wandb.finish()\n",
        "\n",
        "# logClassImages(\"da6401_assignment1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX3u-Z-wedmf",
        "outputId": "fb8ec8d7-278a-446b-fcbb-dc185409a9d9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feedforward neural network\n",
        "\n"
      ],
      "metadata": {
        "id": "6Q1g4A4z5_k0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries required"
      ],
      "metadata": {
        "id": "GYt8MwnejIm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "import copy"
      ],
      "metadata": {
        "id": "n-C62yxXtuau"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activation functions"
      ],
      "metadata": {
        "id": "icnO6u_PiIUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  ACTIVATION FUNCTIONS\n",
        "\"\"\"\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))  # Numerical Stability\n",
        "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)"
      ],
      "metadata": {
        "id": "c1GLWE8XqDav"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Functions"
      ],
      "metadata": {
        "id": "OSZ_TdWoiMwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  LOSS FUNCTIONS\n",
        "\"\"\"\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    return -np.sum(y_true * np.log(y_pred), axis=-1)"
      ],
      "metadata": {
        "id": "oUg5E_A_vemb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Derivatives"
      ],
      "metadata": {
        "id": "dxKR1JYIiQiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  DERIVATIVES OF ACTIVATION AND LOSS FUNCTIONS\n",
        "\"\"\"\n",
        "def identity_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    sig = sigmoid(x)\n",
        "    return sig * (1 - sig)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1 - np.tanh(x) ** 2\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def mean_squared_error_derivative(y_true, y_pred):\n",
        "    return y_pred - y_true\n",
        "\n",
        "def cross_entropy_loss_derivative(y_true, y_pred):\n",
        "    return -y_true / y_pred\n",
        "\n",
        "def softmax_derivative(inp:np.array):\n",
        "    derivates = []\n",
        "    if(len(inp.shape) == 1):\n",
        "      S_vector = inp.reshape(-1, 1)\n",
        "      derivates = np.diag(inp) - np.dot(S_vector, S_vector.T)\n",
        "    elif(len(inp.shape) == 2):\n",
        "      for i in range(inp.shape[0]):\n",
        "        S_vector = inp[i].reshape(-1, 1)\n",
        "        derivates.append(np.diag(inp[i]) - np.dot(S_vector, S_vector.T))\n",
        "\n",
        "    return np.array(derivates)"
      ],
      "metadata": {
        "id": "z7G7FY4D_UN9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizers"
      ],
      "metadata": {
        "id": "pw1-cdOZiWsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  OPTIMIZERS UPDATE RULES\n",
        "\"\"\"\n",
        "\n",
        "# STOCHASTIC GRADIENT DESCENT\n",
        "def sgd(optimizer_input_dict, wts_bias_history_dict, itr=None):\n",
        "  # cant update weights in one single matrix op as dimensions of weights can be different in each layer\n",
        "  for i in range(optimizer_input_dict[\"n_hiddenLayers\"]+1):\n",
        "    # weight decay term added additionally to the formula in slides\n",
        "    wts_bias_history_dict[\"weights\"][i] = wts_bias_history_dict[\"weights\"][i] - optimizer_input_dict[\"learning_rate\"] * (wts_bias_history_dict[\"dw\"][i] + (optimizer_input_dict[\"weight_decay\"] * wts_bias_history_dict[\"weights\"][i]))\n",
        "    wts_bias_history_dict[\"biases\"][i] = wts_bias_history_dict[\"biases\"][i] - optimizer_input_dict[\"learning_rate\"] * wts_bias_history_dict[\"db\"][i]\n",
        "\n",
        "# MOMENTUM BASED GRADIENT DESCENT\n",
        "def momentumGradientDescent(optimizer_input_dict, wts_bias_history_dict, itr=None):\n",
        "  for i in range(optimizer_input_dict[\"n_hiddenLayers\"]+1):\n",
        "    wts_bias_history_dict[\"history_weights\"][i] = (optimizer_input_dict[\"momentum\"] * wts_bias_history_dict[\"history_weights\"][i]) + wts_bias_history_dict[\"dw\"][i] + (optimizer_input_dict[\"weight_decay\"] * wts_bias_history_dict[\"weights\"][i])\n",
        "    wts_bias_history_dict[\"weights\"][i] = wts_bias_history_dict[\"weights\"][i] - optimizer_input_dict[\"learning_rate\"] * wts_bias_history_dict[\"history_weights\"][i]\n",
        "\n",
        "    wts_bias_history_dict[\"history_biases\"][i] = (optimizer_input_dict[\"momentum\"] * wts_bias_history_dict[\"history_biases\"][i]) + wts_bias_history_dict[\"db\"][i]\n",
        "    wts_bias_history_dict[\"biases\"][i] = wts_bias_history_dict[\"biases\"][i] - optimizer_input_dict[\"learning_rate\"] * wts_bias_history_dict[\"history_biases\"][i]\n",
        "\n",
        "# NAG(NESTEROV ACCELERATED GRADIENT DESCENT)\n",
        "def nag(optimizer_input_dict, wts_bias_history_dict, itr=None):\n",
        "  for i in range(optimizer_input_dict[\"n_hiddenLayers\"]+1):\n",
        "    # dw,db will contain lookahead gradients only since forward and backward propagations are implemented accordingly\n",
        "    wts_bias_history_dict[\"history_weights\"][i] = (optimizer_input_dict[\"momentum\"] * wts_bias_history_dict[\"history_weights\"][i]) + wts_bias_history_dict[\"dw\"][i] + (optimizer_input_dict[\"weight_decay\"] * wts_bias_history_dict[\"weights\"][i])\n",
        "    wts_bias_history_dict[\"weights\"][i] = wts_bias_history_dict[\"weights\"][i] - optimizer_input_dict[\"learning_rate\"] * wts_bias_history_dict[\"history_weights\"][i]\n",
        "\n",
        "    wts_bias_history_dict[\"history_biases\"][i] = (optimizer_input_dict[\"momentum\"] * wts_bias_history_dict[\"history_biases\"][i]) + wts_bias_history_dict[\"db\"][i]\n",
        "    wts_bias_history_dict[\"biases\"][i] = wts_bias_history_dict[\"biases\"][i] - optimizer_input_dict[\"learning_rate\"] * wts_bias_history_dict[\"history_biases\"][i]\n",
        "\n",
        "# RMSPROP\n",
        "def rmsProp(optimizer_input_dict, wts_bias_history_dict, itr=None):\n",
        "  for i in range(optimizer_input_dict[\"n_hiddenLayers\"]+1):\n",
        "    wts_bias_history_dict[\"history_weights\"][i] = (optimizer_input_dict[\"beta\"] * wts_bias_history_dict[\"history_weights\"][i]) + (1 - optimizer_input_dict[\"beta\"]) * (wts_bias_history_dict[\"dw\"][i] ** 2)\n",
        "    wts_bias_history_dict[\"weights\"][i] = wts_bias_history_dict[\"weights\"][i] - ((optimizer_input_dict[\"learning_rate\"]/np.sqrt(wts_bias_history_dict[\"history_weights\"][i] + optimizer_input_dict[\"epsilon\"])) * (wts_bias_history_dict[\"dw\"][i] + (optimizer_input_dict[\"weight_decay\"] * wts_bias_history_dict[\"weights\"][i])))\n",
        "\n",
        "    wts_bias_history_dict[\"history_biases\"][i] = (optimizer_input_dict[\"beta\"] * wts_bias_history_dict[\"history_biases\"][i]) + (1 - optimizer_input_dict[\"beta\"]) * (wts_bias_history_dict[\"db\"][i] ** 2)\n",
        "    wts_bias_history_dict[\"biases\"][i] = wts_bias_history_dict[\"biases\"][i] - ((optimizer_input_dict[\"learning_rate\"]/np.sqrt(wts_bias_history_dict[\"history_biases\"][i] + optimizer_input_dict[\"epsilon\"])) * (wts_bias_history_dict[\"db\"][i]))\n",
        "\n",
        "# ADAM\n",
        "def adam(optimizer_input_dict, wts_bias_history_dict, itr=None):\n",
        "  for i in range(optimizer_input_dict[\"n_hiddenLayers\"]+1):\n",
        "    wts_bias_history_dict[\"history_weights\"][i] = (optimizer_input_dict[\"beta1\"] * wts_bias_history_dict[\"history_weights\"][i]) + (1 - optimizer_input_dict[\"beta1\"]) * (wts_bias_history_dict[\"dw\"][i])\n",
        "    wts_bias_history_dict[\"second_history_weights\"][i] = (optimizer_input_dict[\"beta2\"] * wts_bias_history_dict[\"second_history_weights\"][i]) + (1 - optimizer_input_dict[\"beta2\"]) * (wts_bias_history_dict[\"dw\"][i] ** 2)\n",
        "\n",
        "    history_weights_hat = wts_bias_history_dict[\"history_weights\"][i] / (1 - (optimizer_input_dict[\"beta1\"] ** (itr)))\n",
        "    second_history_weights_hat = wts_bias_history_dict[\"second_history_weights\"][i] / (1 - (optimizer_input_dict[\"beta2\"] ** (itr)))\n",
        "\n",
        "    wts_bias_history_dict[\"weights\"][i] = wts_bias_history_dict[\"weights\"][i] - ((optimizer_input_dict[\"learning_rate\"]/(np.sqrt(second_history_weights_hat) + optimizer_input_dict[\"epsilon\"])) * (history_weights_hat + (optimizer_input_dict[\"weight_decay\"] * wts_bias_history_dict[\"weights\"][i])))\n",
        "\n",
        "    wts_bias_history_dict[\"history_biases\"][i] = (optimizer_input_dict[\"beta1\"] * wts_bias_history_dict[\"history_biases\"][i]) + (1 - optimizer_input_dict[\"beta1\"]) * wts_bias_history_dict[\"db\"][i]\n",
        "    wts_bias_history_dict[\"second_history_biases\"][i] = (optimizer_input_dict[\"beta2\"] * wts_bias_history_dict[\"second_history_biases\"][i]) + (1 - optimizer_input_dict[\"beta2\"]) * (wts_bias_history_dict[\"db\"][i] ** 2)\n",
        "\n",
        "    history_biases_hat = wts_bias_history_dict[\"history_biases\"][i] / (1 - (optimizer_input_dict[\"beta1\"] ** (itr)))\n",
        "    second_history_biases_hat = wts_bias_history_dict[\"second_history_biases\"][i] / (1 - (optimizer_input_dict[\"beta2\"] ** (itr)))\n",
        "\n",
        "    wts_bias_history_dict[\"biases\"][i] = wts_bias_history_dict[\"biases\"][i] - ((optimizer_input_dict[\"learning_rate\"]/(np.sqrt(second_history_biases_hat) + optimizer_input_dict[\"epsilon\"])) * (history_biases_hat))\n",
        "\n",
        "# NADAM\n",
        "def nadam(optimizer_input_dict, wts_bias_history_dict, itr=None):\n",
        "  for i in range(optimizer_input_dict[\"n_hiddenLayers\"]+1):\n",
        "    wts_bias_history_dict[\"history_weights\"][i] = (optimizer_input_dict[\"beta1\"] * wts_bias_history_dict[\"history_weights\"][i]) + (1 - optimizer_input_dict[\"beta1\"]) * (wts_bias_history_dict[\"dw\"][i])\n",
        "    wts_bias_history_dict[\"second_history_weights\"][i] = (optimizer_input_dict[\"beta2\"] * wts_bias_history_dict[\"second_history_weights\"][i]) + (1 - optimizer_input_dict[\"beta2\"]) * (wts_bias_history_dict[\"dw\"][i] ** 2)\n",
        "\n",
        "    history_weights_hat = wts_bias_history_dict[\"history_weights\"][i] / (1 - (optimizer_input_dict[\"beta1\"] ** itr))\n",
        "    second_history_weights_hat = wts_bias_history_dict[\"second_history_weights\"][i] / (1 - (optimizer_input_dict[\"beta2\"] ** itr))\n",
        "\n",
        "    lookahead_dw = optimizer_input_dict[\"beta1\"] * history_weights_hat + (((1-optimizer_input_dict[\"beta1\"])/(1-(optimizer_input_dict[\"beta1\"] ** itr))) * wts_bias_history_dict[\"dw\"][i])\n",
        "    wts_bias_history_dict[\"weights\"][i] = wts_bias_history_dict[\"weights\"][i] - ((optimizer_input_dict[\"learning_rate\"]/(np.sqrt(second_history_weights_hat) + optimizer_input_dict[\"epsilon\"])) * (lookahead_dw + (optimizer_input_dict[\"weight_decay\"] * wts_bias_history_dict[\"weights\"][i])))\n",
        "\n",
        "\n",
        "    wts_bias_history_dict[\"history_biases\"][i] = (optimizer_input_dict[\"beta1\"] * wts_bias_history_dict[\"history_biases\"][i]) + (1 - optimizer_input_dict[\"beta1\"]) * wts_bias_history_dict[\"db\"][i]\n",
        "    wts_bias_history_dict[\"second_history_biases\"][i] = (optimizer_input_dict[\"beta2\"] * wts_bias_history_dict[\"second_history_biases\"][i]) + (1 - optimizer_input_dict[\"beta2\"]) * (wts_bias_history_dict[\"db\"][i] ** 2)\n",
        "\n",
        "    history_biases_hat = wts_bias_history_dict[\"history_biases\"][i] / (1 - (optimizer_input_dict[\"beta1\"] ** itr))\n",
        "    second_history_biases_hat = wts_bias_history_dict[\"second_history_biases\"][i] / (1 - (optimizer_input_dict[\"beta2\"] ** itr))\n",
        "\n",
        "    lookahead_db = optimizer_input_dict[\"beta1\"] * history_biases_hat + (((1-optimizer_input_dict[\"beta1\"])/(1-(optimizer_input_dict[\"beta1\"] ** itr))) * wts_bias_history_dict[\"db\"][i])\n",
        "    wts_bias_history_dict[\"biases\"][i] = wts_bias_history_dict[\"biases\"][i] - ((optimizer_input_dict[\"learning_rate\"]/(np.sqrt(second_history_biases_hat) + optimizer_input_dict[\"epsilon\"])) * (lookahead_db))\n",
        ""
      ],
      "metadata": {
        "id": "BuetfNNzUvjT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network (forward and back propagation)"
      ],
      "metadata": {
        "id": "TOWr8-ojiZ1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNeuralNetwork:\n",
        "  # class variables\n",
        "  optimizersMap = {\"sgd\": sgd, \"momentum\": momentumGradientDescent, \"nag\": nag, \"rmsprop\": rmsProp, \"adam\": adam, \"nadam\": nadam}\n",
        "  lossFunctionsMap = {\"mean_squared_error\": mean_squared_error, \"cross_entropy\" : cross_entropy_loss}\n",
        "  activationFunctionsMap = {\"identity\":identity, \"sigmoid\":sigmoid, \"tanh\":tanh, \"ReLU\":relu, \"softmax\": softmax}\n",
        "  derivatesFuncMap = {\"mean_squared_error\": mean_squared_error_derivative, \"cross_entropy_loss\": cross_entropy_loss_derivative, \"identity\": identity_derivative,\n",
        "                      \"sigmoid\": sigmoid_derivative, \"tanh\": tanh_derivative, \"relu\": relu_derivative, \"softmax\": softmax_derivative}\n",
        "\n",
        "  def __init__(self,\n",
        "               input_size=784, output_size=10,\n",
        "               n_hiddenLayers=3, n_neuronsPerLayer=32,\n",
        "               activationFun=\"sigmoid\",\n",
        "               weight_init=\"random\",\n",
        "               batch_size=64,\n",
        "               lossFunc=\"cross_entropy\",\n",
        "               optimizer=\"adam\",\n",
        "               learning_rate=0.001,\n",
        "               momentum=0.5,\n",
        "               beta=0.9, beta1=0.9, beta2=0.99,\n",
        "               epsilon=1e-8, weight_decay=0.01,\n",
        "               epochs=10):\n",
        "\n",
        "    # Inialtization parameters\n",
        "    self.input_size = input_size  # no of features\n",
        "    self.output_size = output_size\n",
        "    self.n_hiddenLayers = n_hiddenLayers\n",
        "    self.n_neuronsPerLayer = n_neuronsPerLayer\n",
        "    self.weight_init = weight_init\n",
        "    self.epochs = epochs\n",
        "\n",
        "    self.activationFun = FeedForwardNeuralNetwork.activationFunctionsMap[activationFun]\n",
        "    self.lossFunc = FeedForwardNeuralNetwork.lossFunctionsMap[lossFunc]\n",
        "    self.optimizer = FeedForwardNeuralNetwork.optimizersMap[optimizer]\n",
        "\n",
        "    # paramters required for optimizers\n",
        "    self.batch_size = batch_size\n",
        "    self.isLookAhead = False;\n",
        "\n",
        "    if(optimizer == \"nag\"):\n",
        "      self.isLookAhead = True;\n",
        "\n",
        "    # add these parameters as dict\n",
        "    self.optimizer_input_dict = { \"learning_rate\" : learning_rate,\n",
        "                                  \"momentum\" : momentum,                  # used by momentumGD\n",
        "                                  \"beta\" : beta,                          # used by rmsprop\n",
        "                                  \"beta1\" : beta1,                        # used by adam & nadam\n",
        "                                  \"beta2\" : beta2,                        # used by adam & nadam\n",
        "                                  \"epsilon\" : epsilon,\n",
        "                                  \"weight_decay\" : weight_decay,\n",
        "                                  \"n_hiddenLayers\": n_hiddenLayers}\n",
        "\n",
        "    # weights and biases matrices\n",
        "    self.weights = []\n",
        "    self.biases = []\n",
        "    self.lookAheadWeights = []\n",
        "    self.lookAheadBiases = []\n",
        "\n",
        "    self.wts_bias_history_dict = {\"weights\": self.weights, \"biases\": self.biases,\n",
        "                                  \"history_weights\": [np.zeros(1) for _ in range(self.n_hiddenLayers+1)],         # these will be modified before their first use (dimensions of each values will also be changed)\n",
        "                                  \"history_biases\": [np.zeros(1) for _ in range(self.n_hiddenLayers+1)],\n",
        "                                  \"second_history_weights\": [np.zeros(1) for _ in range(self.n_hiddenLayers+1)],\n",
        "                                  \"second_history_biases\": [np.zeros(1) for _ in range(self.n_hiddenLayers+1)],\n",
        "                                  \"dw\": [np.empty(1) for _ in range(self.n_hiddenLayers+1)],\n",
        "                                  \"dh\": [np.empty(1) for _ in range(self.n_hiddenLayers+1)]}\n",
        "\n",
        "    self.initializeWeightsAndBiases()\n",
        "    self.wts_bias_history_dict[\"second_history_weights\"] = copy.deepcopy(self.wts_bias_history_dict[\"history_weights\"])\n",
        "    self.wts_bias_history_dict[\"second_history_biases\"] = copy.deepcopy(self.wts_bias_history_dict[\"history_biases\"])\n",
        "\n",
        "    # pre-activation(a) and post-activation(h) values\n",
        "    self.a = []\n",
        "    self.h = []\n",
        "\n",
        "  '''\n",
        "    Weights,Biases initialization based on weight_init parameter\n",
        "\n",
        "    weights[0]: input layer to first hidden layer  : input_size x n_neuronsPerLayer\n",
        "    weights[1]: first hidden layer to second hidden layer : n_neuronsPerLayer x n_neuronsPerLayer\n",
        "    ...\n",
        "    weights[n_hiddenLayers]: last hidden layer to output layer : n_neuronsPerLayer x output_size\n",
        "\n",
        "    biases[i] : bias for ith layer : 1 x n_neuronsPerLayer   (i:0 to n_hiddenLayers-1)\n",
        "    biases[n_hiddenLayers]: 1 x output_size\n",
        "  '''\n",
        "  def initializeWeightsAndBiases(self):\n",
        "    # biases for both types\n",
        "    for i in range(self.n_hiddenLayers):\n",
        "      self.biases.append(np.zeros(self.n_neuronsPerLayer))\n",
        "      self.wts_bias_history_dict[\"history_biases\"][i] = np.zeros(self.n_neuronsPerLayer)\n",
        "\n",
        "    self.biases.append(np.zeros(self.output_size))   # biases[n_hiddenLayers]\n",
        "    self.wts_bias_history_dict[\"history_biases\"][self.n_hiddenLayers] = np.zeros(self.output_size)\n",
        "\n",
        "    if(self.weight_init == \"random\"):   # Random Normal\n",
        "      # weights[0]\n",
        "      self.weights.append(np.random.randn(self.input_size, self.n_neuronsPerLayer))\n",
        "      self.wts_bias_history_dict[\"history_weights\"][0] = np.zeros((self.input_size, self.n_neuronsPerLayer))\n",
        "\n",
        "      # weights[1] -> weights[n_hiddenLayers-1]\n",
        "      for i in range(self.n_hiddenLayers-1):\n",
        "        self.weights.append(np.random.randn(self.n_neuronsPerLayer, self.n_neuronsPerLayer))\n",
        "        self.wts_bias_history_dict[\"history_weights\"][i+1] = np.zeros((self.n_neuronsPerLayer, self.n_neuronsPerLayer))\n",
        "\n",
        "      # weights[n_hiddenLayers]\n",
        "      self.weights.append(np.random.randn(self.n_neuronsPerLayer, self.output_size))\n",
        "      self.wts_bias_history_dict[\"history_weights\"][self.n_hiddenLayers] = np.zeros((self.n_neuronsPerLayer, self.output_size))\n",
        "\n",
        "    elif(self.weight_init == \"Xavier\"):   # Xavier Normal: mean = 0, variance = 2/(n_input + n_output)\n",
        "      # weights[0]\n",
        "      self.weights.append(np.random.normal(loc=0.0, scale=np.sqrt(2/(self.input_size + self.n_neuronsPerLayer)), size=(self.input_size, self.n_neuronsPerLayer)))\n",
        "      self.wts_bias_history_dict[\"history_weights\"][0] = np.zeros((self.input_size, self.n_neuronsPerLayer))\n",
        "\n",
        "\n",
        "      for i in range(self.n_hiddenLayers-1):\n",
        "        self.weights.append(np.random.normal(loc=0.0, scale=np.sqrt(2/(self.n_neuronsPerLayer + self.n_neuronsPerLayer)), size=(self.n_neuronsPerLayer, self.n_neuronsPerLayer)))\n",
        "        self.wts_bias_history_dict[\"history_weights\"][i+1] = np.zeros((self.n_neuronsPerLayer, self.n_neuronsPerLayer))\n",
        "\n",
        "\n",
        "      self.weights.append(np.random.normal(loc=0.0, scale=np.sqrt(2/(self.n_neuronsPerLayer + self.output_size)), size=(self.n_neuronsPerLayer, self.output_size)))\n",
        "      self.wts_bias_history_dict[\"history_weights\"][self.n_hiddenLayers] = np.zeros((self.n_neuronsPerLayer, self.output_size))\n",
        "\n",
        "  '''\n",
        "    Forward propagation through the neural network. (for batch)\n",
        "    Instead of doing one input at a time, this function handles it for a batch using respective sized matrices\n",
        "\n",
        "    x_batch: B x n where B - batch size, n- no of features = input_size\n",
        "    x_batch is assumbed to be numpy array when given as input\n",
        "  '''\n",
        "  def forwardPropagation(self, x_batch):\n",
        "    a_pre_activation = []\n",
        "    h_post_activation = []\n",
        "\n",
        "    # considering a0,h0 as X values as a1: first layer  (it is calculated from x values)\n",
        "    a_pre_activation.append(x_batch)\n",
        "    h_post_activation.append(x_batch)\n",
        "\n",
        "    wt = []\n",
        "    b = []\n",
        "\n",
        "    if(self.isLookAhead):\n",
        "      for i in range(self.n_hiddenLayers+1):\n",
        "        wt.append(self.weights[i] - (self.optimizer_input_dict[\"momentum\"] * self.wts_bias_history_dict[\"history_weights\"][i]))\n",
        "        b.append(self.biases[i] - (self.optimizer_input_dict[\"momentum\"] * self.wts_bias_history_dict[\"history_biases\"][i]))\n",
        "\n",
        "      self.lookAheadWeights = wt\n",
        "      self.lookAheadBiases = b\n",
        "    else:\n",
        "      wt = copy.deepcopy(self.weights)\n",
        "      b = copy.deepcopy(self.biases)\n",
        "\n",
        "    # Except last layer since activation function could be different\n",
        "    for i in range(self.n_hiddenLayers):\n",
        "      # ai: B x n_neuronsPerLayer, biases[i]: 1 x n_neuronsPerLayer (it will be broadcasted while adding)\n",
        "      ai = np.matmul(h_post_activation[-1], wt[i]) + b[i]\n",
        "      hi = self.activationFun(ai)\n",
        "\n",
        "      a_pre_activation.append(ai)\n",
        "      h_post_activation.append(hi)\n",
        "\n",
        "    # aL: last layer (activation function is softmax)\n",
        "    aL = np.matmul(h_post_activation[-1], wt[self.n_hiddenLayers]) + b[self.n_hiddenLayers]\n",
        "    hL = softmax(aL)   # y_batch\n",
        "\n",
        "    a_pre_activation.append(aL)\n",
        "    h_post_activation.append(hL)\n",
        "\n",
        "    return a_pre_activation, h_post_activation\n",
        "\n",
        "  '''\n",
        "    Backward propagation through the neural network. (for batch)\n",
        "  '''\n",
        "  def backwardPropagation(self, a_pre_activation, h_post_activation, y_batch, y_pred_batch):\n",
        "    grad_w = []\n",
        "    grad_b = []\n",
        "    grad_a = []\n",
        "    grad_h = []\n",
        "\n",
        "    # Output gradient (wrt aL)\n",
        "    grad_hL = self.derivatesFuncMap(self.lossFunc)(y_batch, y_pred_batch)\n",
        "    grad_h.append(grad_hL)\n",
        "\n",
        "    if(self.lossFunc.__name__ == \"cross_entropy_loss\"):\n",
        "      grad_aL = y_pred_batch - y_batch    # just to reduce computation of jacobian matrix\n",
        "    else:\n",
        "      grad_aL_list = []\n",
        "      # softmax derivatives of each input is a matrix of size output_size x output_size, we need to perform matrix_mul for each input of batch\n",
        "      for i in range(y_batch.shape[0]):   # self.batch_size = y_batch.shape[0] but better to take y_batch.shape[0] since last batch inputs can have less\n",
        "        grad_aL_inp_i = grad_hL[i] @ softmax_derivative(y_pred_batch[i])\n",
        "        grad_aL_list.append(grad_aL_inp_i)\n",
        "\n",
        "      grad_aL = np.array(grad_aL_list)\n",
        "      grad_a.append(grad_aL)                    # aL contains (aL) values of all inputs in the batch\n",
        "\n",
        "    # Hidden layers\n",
        "    for k in range(self.n_hiddenLayers, -1, -1):\n",
        "      # gradients w.r.t parameters\n",
        "      # wk\n",
        "      grad_wk = np.zeros_like(self.weights[k])    # will be equal to sum across\n",
        "\n",
        "      for inpNum in range(y_batch.shape[0]):\n",
        "        grad_wk_inp_num = np.matmul(h_post_activation[k][inpNum].reshape(-1,1), grad_a[-1][inpNum].reshape(1,-1))\n",
        "        grad_wk += grad_wk_inp_num\n",
        "      grad_w.append(grad_wk)                   # contains sum across all batches\n",
        "\n",
        "      # bk\n",
        "      grad_bk = np.zeros_like(self.biases[k])\n",
        "      for inpNum in range(y_batch.shape[0]):\n",
        "        grad_bk += grad_a[-1][inpNum]\n",
        "      grad_b.append(grad_bk)                     # contains sum across all batches\n",
        "\n",
        "      if(k > 0):\n",
        "        # gradients w.r.t layer below\n",
        "        grad_hk_1 = grad_a[-1] @ self.weights[k].T\n",
        "        grad_h.append(grad_hk_1)\n",
        "\n",
        "        # gradients w.r.t layer below (pre-activation)\n",
        "        grad_ak_1 = grad_hk_1 * self.derivatesFuncMap(self.activationFun.__name__)(a_pre_activation[k])\n",
        "        grad_a.append(grad_ak_1)\n",
        "\n",
        "    grad_w = grad_w[::-1]\n",
        "    grad_b = grad_b[::-1]\n",
        "\n",
        "    return grad_w, grad_b"
      ],
      "metadata": {
        "id": "OAFNdkiD6ORe"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading data"
      ],
      "metadata": {
        "id": "Sn4gQPUbjAXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist, mnist\n",
        "import numpy as np\n",
        "\n",
        "datasets = {\"fashion_mnist\": fashion_mnist, \"mnist\": mnist}\n",
        "\n",
        "def load_data(dataset_name):\n",
        "  (x_train, y_train), (x_test, y_test) = datasets[dataset_name].load_data()\n",
        "  num_classes = len(np.unique(y_train))\n",
        "\n",
        "  y_train = np.eye(num_classes)[y_train]\n",
        "  y_test = np.eye(num_classes)[y_test]\n",
        "\n",
        "  x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "  x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "  x_train = x_train/255\n",
        "  y_train = y_train/255\n",
        "\n",
        "  return x_train, y_train, x_test, y_test"
      ],
      "metadata": {
        "id": "NrmZMAZQjGDg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Feed Forward Neural Network"
      ],
      "metadata": {
        "id": "2DJr_18yixYa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C5Hv_3X6i3UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random tests"
      ],
      "metadata": {
        "id": "1eohFAdEil5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# # Create a column vector (shape: (3,1))\n",
        "# v1 = np.array([[1,2], [2,3], [3,4]])  # Shape (3,1)\n",
        "# v2 = np.array([[4], [5], [6]])  # Shape (3,1)\n",
        "\n",
        "# # Append to a list\n",
        "# vector_list = []\n",
        "# vector_list.append(v1)\n",
        "# vector_list.append(v2)\n",
        "\n",
        "# print(vector_list)\n",
        "\n",
        "# # flatten each element of x_train, after flattening store it as np array\n",
        "# x_ftrain = np.array([x.flatten() for x in x_train])\n",
        "# x1 = [x.flatten() for x in x_train]\n",
        "# # print(x_ftrain[0].shape)\n",
        "# # print(x_ftrain.shape)\n",
        "# # print(x_ftrain[:20].shape)\n",
        "# # print(y_train[:20].shape)\n",
        "# # print(x_test.shape)\n",
        "# # print(y_test.shape)\n",
        "# m = np.array(np.zeros((784,3)))\n",
        "# y = np.matmul(x_ftrain[:20],m)\n",
        "# print(y.shape)\n",
        "\n",
        "\n",
        "y_batch = np.array([\n",
        "    [0, 1, 0.5],  # Example 1\n",
        "    [1, 0, 0],  # Example 2\n",
        "    [0, 0, 1]   # Example 3\n",
        "])\n",
        "\n",
        "y_hat_batch = np.array([\n",
        "    [0.2, 0.5, 0.3],\n",
        "    [0.8, 0.1, 0.1],\n",
        "    [0.3, 0.3, 0.4]\n",
        "])\n",
        "\n",
        "grad_batch = - (y_batch / y_hat_batch)  # Element-wise division\n",
        "print(grad_batch)\n"
      ],
      "metadata": {
        "id": "Ymap_UxJJhHb",
        "outputId": "8bddff6a-f054-446c-a8c8-2c3c4543f487",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.         -2.         -1.66666667]\n",
            " [-1.25       -0.         -0.        ]\n",
            " [-0.         -0.         -2.5       ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "def identity_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "x_single = 2.0\n",
        "x_vector = np.array([1.0, 2.0, -3.0])\n",
        "x_batch = np.array([[1.0, 1.0], [3.0, 4.0]])\n",
        "\n",
        "print(identity(x_batch))  # Works\n",
        "print(identity_derivative(x_batch))  # Works\n"
      ],
      "metadata": {
        "id": "npduqOsCwoRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    sig = sigmoid(x)\n",
        "    return sig * (1 - sig)\n",
        "\n",
        "print(sigmoid(x_vector))  # Works\n",
        "print(sigmoid_derivative(x_vector))  # Works"
      ],
      "metadata": {
        "id": "fr4DlGPEw1XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1 - np.tanh(x) ** 2\n",
        "\n",
        "print(tanh(x_vector))  # Works\n",
        "print(tanh_derivative(x_single))  # Works"
      ],
      "metadata": {
        "id": "MmN5bYRGx_ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "print(relu(x_vector))  # Works\n",
        "print(relu_derivative(x_vector))  # Works"
      ],
      "metadata": {
        "id": "lebRmhFfyFzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # def softmax(x):\n",
        "# #     exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))  # Stability trick\n",
        "# #     return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "\n",
        "# # Softmax needs a vector input\n",
        "# x_vector = np.array([1.0, 20000.0, 3.0])\n",
        "# x_batch = np.array([[1.0, 2.0, 3000.0], [1.5, 2.5, 3.5]])\n",
        "\n",
        "# print(softmax(x_vector))  # Works\n",
        "# print(softmax(x_batch))  # Works (batch-wise softmax)"
      ],
      "metadata": {
        "id": "saSGnq50y3Px"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COQq9E_90BZA",
        "outputId": "0b64bdd1-0ae2-47c5-ff18-8cf4f73eea79"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9 0 0 ... 3 0 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "num_classes = 10\n",
        "y_train_one_hot = np.eye(num_classes)[y_train]\n",
        "y_test_one_hot = np.eye(num_classes)[y_test]\n",
        "\n",
        "print(y_train_one_hot[-1])  # Output: (num_samples, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdvhmcUv0Y5S",
        "outputId": "fa1973df-9441-4ce9-972a-147139968da2"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "e-4bxy_g5ds8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_vector)\n",
        "x_vector=[1,2,3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCntCjaE60aa",
        "outputId": "5cbcc511-728d-459e-e8e0-d5e8a26d76c6"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.  2. -3.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true_single = np.array([0, 0, 1])  # One-hot encoded true label\n",
        "y_pred_single = np.array([0.1, 0.7, 0.2])\n",
        "cross_entropy_loss_derivative(y_true_single, y_pred_single)\n",
        "\n",
        "y_true_batch = np.array([[0, 1, 0], [1, 0, 0]])  # One-hot encoded true labels for two examples\n",
        "y_pred_batch = np.array([[0.1, 0.7, 0.2], [0.8, 0.1, 0.1]])\n",
        "cross_entropy_loss_derivative(y_true_batch, y_pred_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAxzfNm646GZ",
        "outputId": "b789622b-026b-4f1e-9497-c3e1d2cd2865"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        , -1.42857143,  0.        ],\n",
              "       [-1.25      ,  0.        ,  0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = softmax_derivative(y_pred_single)\n",
        "print(x)\n",
        "print(x.shape)\n",
        "\n",
        "y = softmax_derivative(y_pred_batch)\n",
        "print(y)\n",
        "print(y[0].shape)\n",
        "print(y.shape)\n",
        "# len(y_true_batch.shape)"
      ],
      "metadata": {
        "id": "zxFtQGGNE-a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(dictx):\n",
        "  dictx[\"key\"] = 1\n",
        "  dictx[\"dgy\"] = 2\n",
        "  print(dictx)\n",
        "\n",
        "dicty = {}\n",
        "f(dicty)\n",
        "print(dicty)\n",
        "dicty[\"2\"] = 2\n",
        "f(dicty)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijle4EU6vkxp",
        "outputId": "322a8dae-e735-4986-9260-64d51d5f6e91"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'key': 1, 'dgy': 2}\n",
            "{'key': 1, 'dgy': 2}\n",
            "{'key': 1, 'dgy': 2, '2': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    return -np.sum(y_true * np.log(y_pred), axis=-1)\n",
        "\n",
        "def cross_entropy_loss_derivative(y_true, y_pred):\n",
        "    return -y_true / y_pred  # Element-wise division\n",
        "\n",
        "def softmax_derivative(inp: np.array):\n",
        "    derivates = []\n",
        "    if len(inp.shape) == 1:\n",
        "        S_vector = inp.reshape(-1, 1)\n",
        "        derivates = np.diag(inp) - np.dot(S_vector, S_vector.T)\n",
        "    elif len(inp.shape) == 2:\n",
        "        for i in range(inp.shape[0]):\n",
        "            S_vector = inp[i].reshape(-1, 1)\n",
        "            derivates.append(np.diag(inp[i]) - np.dot(S_vector, S_vector.T))\n",
        "    return np.array(derivates)\n",
        "\n",
        "# Sample data\n",
        "y_true = np.array([[0, 1, 0], [1, 0, 0]])  # One-hot encoded true labels for 2 samples, 3 classes\n",
        "y_pred = np.array([[0.1, 0.8, 0.1], [0.7, 0.2, 0.1]])  # Predicted probabilities from softmax\n",
        "\n",
        "# Derivatives\n",
        "grad_hL = cross_entropy_loss_derivative(y_true, y_pred)  # Gradient of cross-entropy loss\n",
        "print(grad_hL)\n",
        "\n",
        "# Assume the activation function is softmax, and we have computed the pre-activation (a_pre_activation[-1])\n",
        "# Use softmax derivative for the last layer\n",
        "for i in range(y_true.shape[0]):\n",
        "    grad_aL = grad_hL[i] @ softmax_derivative(y_pred[i])  # Element-wise product\n",
        "    print(grad_aL)\n",
        "# grad_aL = grad_hL * softmax_derivative(y_pred)  # Element-wise product\n",
        "# print(grad_aL)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPbFl70_vku3",
        "outputId": "e2fdfcaf-861f-42e5-e2f5-81e27a6f796d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.         -1.25        0.        ]\n",
            " [-1.42857143  0.          0.        ]]\n",
            "[ 0.1 -0.2  0.1]\n",
            "[-0.3  0.2  0.1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(x_train))\n",
        "print(x_train[0].flatten())"
      ],
      "metadata": {
        "id": "1W3aoMDGkkcZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
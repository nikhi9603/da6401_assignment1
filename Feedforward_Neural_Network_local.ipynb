{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "RyBmHFH18zVt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ColG2_IA5yF7",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "pip install wandb numpy pandas matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1: fashion-MNIST dataset"
      ],
      "metadata": {
        "id": "7OazPdR79C0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "classes = {\n",
        "    0: \"T-shirt/top\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle boot\"\n",
        "}\n",
        "\n",
        "def logClassImages(project_name:str):\n",
        "  wandb.init(project=project_name)\n",
        "  wandb_image_indices = []\n",
        "\n",
        "  for classNumber in range(10):\n",
        "    for j in range(len(y_test)):\n",
        "      if y_test[j] == classNumber:\n",
        "        wandb_image_indices.append(x_test[j])\n",
        "        break\n",
        "\n",
        "  wandb_images = [wandb.Image(wandb_image_indices[i], caption = classes[i]) for i in range(10)]\n",
        "  wandb.log({\"Sample images for each class\": wandb_images})\n",
        "  wandb.finish()\n",
        "\n",
        "# logClassImages(\"da6401_assignment1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX3u-Z-wedmf",
        "outputId": "0ef4a636-b212-4a75-f279-090f8a52707f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feedforward neural network\n",
        "\n"
      ],
      "metadata": {
        "id": "6Q1g4A4z5_k0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "n-C62yxXtuau"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation functions:\n",
        "def softmax(x):\n",
        "  pass"
      ],
      "metadata": {
        "id": "c1GLWE8XqDav"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNeuralNetwork:\n",
        "  # class variables\n",
        "  optimizersMap = {\"a\":lambda x: x+1}\n",
        "  lossFunctionsMap = {\"a\":lambda x: x+1}\n",
        "  activationFunctionsMap = {\"a\":lambda x: x+1}\n",
        "\n",
        "  def __init__(self,\n",
        "               input_size=784, output_size=10,\n",
        "               n_hiddenLayers=3, n_neuronsPerLayer=32,\n",
        "               activationFun=\"sigmoid\",\n",
        "               weight_init=\"random\",\n",
        "               batch_size=64,\n",
        "               lossFunc=\"cross_entropy\",\n",
        "               optimizer=\"adam\",\n",
        "               learning_rate=0.001,\n",
        "               momentum=0.5,\n",
        "               beta=0.9, beta1=0.9, beta2=0.99,\n",
        "               epsilon=1e-8, weight_decay=0.01,\n",
        "               epochs=10):\n",
        "\n",
        "    # Inialtization parameters\n",
        "    self.input_size = input_size  # no of features\n",
        "    self.output_size = output_size\n",
        "    self.n_hiddenLayers = n_hiddenLayers\n",
        "    self.n_neuronsPerLayer = n_neuronsPerLayer\n",
        "    self.weight_init = weight_init\n",
        "    self.epochs = epochs\n",
        "\n",
        "    self.activationFun = FeedForwardNeuralNetwork.activationFunctionsMap[activationFun]\n",
        "    self.lossFunc = FeedForwardNeuralNetwork.lossFunctionsMap[lossFunc]\n",
        "    self.optimizer = FeedForwardNeuralNetwork.optimizersMap[optimizer]\n",
        "\n",
        "    # paramters required for optimizers\n",
        "    self.batch_size = batch_size\n",
        "    self.learning_rate = learning_rate\n",
        "    self.momentum = momentum\n",
        "    self.beta = beta\n",
        "    self.beta1 = beta1\n",
        "    self.beta2 = beta2\n",
        "    self.epsilon = epsilon\n",
        "    self.weight_decay = weight_decay\n",
        "\n",
        "    # weights and biases matrices\n",
        "    self.weights = []\n",
        "    self.biases = []\n",
        "    self.initializeWeightsAndBiases()\n",
        "\n",
        "    # pre-activation(a) and post-activation(h) values\n",
        "    self.a = []\n",
        "    self.h = []\n",
        "\n",
        "  '''\n",
        "    Weights,Biases initialization based on weight_init parameter\n",
        "\n",
        "    weights[0]: input layer to first hidden layer  : input_size x n_neuronsPerLayer\n",
        "    weights[1]: first hidden layer to second hidden layer : n_neuronsPerLayer x n_neuronsPerLayer\n",
        "    ...\n",
        "    weights[n_hiddenLayers]: last hidden layer to output layer : n_neuronsPerLayer x output_size\n",
        "\n",
        "    biases[i] : bias for ith layer : 1 x n_neuronsPerLayer   (i:0 to n_hiddenLayers-1)\n",
        "    biases[n_hiddenLayers]: 1 x output_size\n",
        "  '''\n",
        "  def initializeWeightsAndBiases(self):\n",
        "    # biases for both types\n",
        "    for _ in range(self.n_hiddenLayers):\n",
        "      self.biases.append(np.zeros(self.n_neuronsPerLayer))\n",
        "    self.biases.append(np.zeros(self.output_size))   # biases[n_hiddenLayers]\n",
        "\n",
        "    if(self.weight_init == \"random\"):   # Random Normal\n",
        "      # weights[0]\n",
        "      self.weights.append(np.random.randn(self.input_size, self.n_neuronsPerLayer))\n",
        "\n",
        "      # weights[1] -> weights[n_hiddenLayers-1]\n",
        "      for _ in range(self.n_hiddenLayers-1):\n",
        "        self.weights.append(np.random.randn(self.n_neuronsPerLayer, self.n_neuronsPerLayer))\n",
        "\n",
        "      # weights[n_hiddenLayers]\n",
        "      self.weights.append(np.random.randn(self.n_neuronsPerLayer, self.output_size))\n",
        "    elif(self.weight_init == \"Xavier\"):   # Xavier Normal: mean = 0, variance = 2/(n_input + n_output)\n",
        "      # weights[0]\n",
        "      self.weights.append(np.random.normal(loc=0.0, scale=np.sqrt(2/(self.input_size + self.n_neuronsPerLayer)), size=(self.input_size, self.n_neuronsPerLayer)))\n",
        "\n",
        "      for _ in range(self.n_hiddenLayers-1):\n",
        "        self.weights.append(np.random.normal(loc=0.0, scale=np.sqrt(2/(self.n_neuronsPerLayer + self.n_neuronsPerLayer)), size=(self.n_neuronsPerLayer, self.n_neuronsPerLayer)))\n",
        "\n",
        "      self.weights.append(np.random.normal(loc=0.0, scale=np.sqrt(2/(self.n_neuronsPerLayer + self.output_size)), size=(self.n_neuronsPerLayer, self.output_size)))\n",
        "\n",
        "  '''\n",
        "    Forward propagation through the neural network. (for batch)\n",
        "    Instead of doing one input at a time, this function handles it for a batch using respective sized matrices\n",
        "\n",
        "    x_batch: B x n where B - batch size, n- no of features = input_size\n",
        "    x_batch is assumbed to be numpy array when given as input\n",
        "  '''\n",
        "  def forwardPropagation(self, x_batch):\n",
        "    a_pre_activation = []\n",
        "    h_post_activation = []\n",
        "\n",
        "    # considering a0,h0 as X values as a1: first layer  (it is calculated from x values)\n",
        "    a_pre_activation.append(x_batch)\n",
        "    h_post_activation.append(x_batch)\n",
        "\n",
        "    # Except last layer since activation function could be different\n",
        "    for i in range(self.n_hiddenLayers):\n",
        "      # ai: B x n_neuronsPerLayer, biases[i]: 1 x n_neuronsPerLayer (it will be broadcasted while adding)\n",
        "      ai = np.matmul(h_post_activation[-1], self.weights[i]) + self.biases[i]\n",
        "      hi = self.activationFun(ai)\n",
        "\n",
        "      a_pre_activation.append(ai)\n",
        "      h_post_activation.append(hi)\n",
        "\n",
        "    # aL: last layer (activation function is softmax)\n",
        "    aL = np.matmul(h_post_activation[-1], self.weights[self.n_hiddenLayers]) + self.biases[self.n_hiddenLayers]\n",
        "    hL = softmax(aL)\n",
        "\n",
        "    a_pre_activation.append(aL)\n",
        "    h_post_activation.append(hL)\n",
        "\n",
        "    return a_pre_activation, h_post_activation"
      ],
      "metadata": {
        "id": "OAFNdkiD6ORe"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "\n",
        "# # Create a column vector (shape: (3,1))\n",
        "# v1 = np.array([[1,2], [2,3], [3,4]])  # Shape (3,1)\n",
        "# v2 = np.array([[4], [5], [6]])  # Shape (3,1)\n",
        "\n",
        "# # Append to a list\n",
        "# vector_list = []\n",
        "# vector_list.append(v1)\n",
        "# vector_list.append(v2)\n",
        "\n",
        "# print(vector_list)\n",
        "\n",
        "# # flatten each element of x_train, after flattening store it as np array\n",
        "# x_ftrain = np.array([x.flatten() for x in x_train])\n",
        "# x1 = [x.flatten() for x in x_train]\n",
        "# # print(x_ftrain[0].shape)\n",
        "# # print(x_ftrain.shape)\n",
        "# # print(x_ftrain[:20].shape)\n",
        "# # print(y_train[:20].shape)\n",
        "# # print(x_test.shape)\n",
        "# # print(y_test.shape)\n",
        "# m = np.array(np.zeros((784,3)))\n",
        "# y = np.matmul(x_ftrain[:20],m)\n",
        "# print(y.shape)"
      ],
      "metadata": {
        "id": "Ymap_UxJJhHb",
        "outputId": "b1fac802-b8e9-44f2-a474-e31f1675ab5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[1, 2],\n",
            "       [2, 3],\n",
            "       [3, 4]]), array([[4],\n",
            "       [5],\n",
            "       [6]])]\n",
            "(20, 3)\n"
          ]
        }
      ]
    }
  ]
}